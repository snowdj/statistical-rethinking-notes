# Chapter 7 - Ulysses' Compass
```{r, include=FALSE}
library(rethinking)
```

Ockham's/Occam's Razor - Models with fewer assumptions are to be preferred. Not usually what we run into though - more often trade-off is accuracy vs simplicity. This chapter goes over tools for the trade-off.

A couple major errors:

_Overfitting_ - Learning too much from data, being over-constrained

_Underfitting_ - Not learning enough, under-constrained.

Also _confounding_ in prior chapters. In this chapter, will explore navigating these via _regularizing priors_ and _information criteria_ or _cross validation_.

For information criteria, also will introduce _information theory_.

## 7.1 - The Problem with Parameters

Prior chapters showed adding variables can hurt causal models, but also problematic for just predictive ones too. 

"Fit" in book means "how well the model can retrodict data used to fit the model

$R^2$ is most common metric - "Variance explained"

$$
R^2 = \frac{\text{var}(\text{outcome}) - \text{var}(\text{residuals})}{\text{var}(\text{outcome})}  = 1- \frac{\text{var}(\text{residuals})}{\text{var}(\text{outcome})}
$$

Problem is that this will always increase as you add more parameters.

### 7.1.1 More parameters will (almost) always improve fit

Occurs when a model learns too much from a sample - both _regular_ and _irregular_ features in every sample. Regular features - targets of learning, generalize well. Irregular - aspects that do not generalize and mislead.

Example - brain volumes and body masses (note, making a data frame from scratch so learning code here too)

```{r}
sppnames <- c( "afarensis","africanus","habilis","boisei", "rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
```

First rescale variables, body mass to $\mu=0$ and $\sigma=1$. Brain size rescale so largest is 1 (don't standardize because negative brain doesn't exist).

```{r}
d$mass_std <- (d$mass - mean(d$mass))/sd(d$mass) 
d$brain_std <- d$brain / max(d$brain)
```

Fit increasingly complex models to this to see effect of overly complex model. 

Start simple linear, brain volume $b_i$ is linear of body mass $m_i$

\begin{align*}
  b_i &\sim \text{Normal}(\mu_i, \sigma)\\
  \mu_i &= \alpha + \beta m_i\\
  \alpha &\sim \text{Normal}(0.5,1)\\
  \beta &\sim \text{Normal}(0,10)\\
  \sigma &\sim \text{Log-Normal}(0,1)
\end{align*}

Ridiculously wide priors - body mass has 89% credible interval from -1 to 2, $\beta$ is flat and centered on zero.

```{r}
m7.1 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ), 
    mu <- a + b*mass_std,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 ) ), 
  data=d )
```

Look at $R^2$, variance "explained" by the model - in other words, model retrodicts some proportion of total variation on outcome data, remaining is variation of residuals. Note, we're trying to argue against $R^2$ here. Compute posterior with sim.

```{r}
set.seed(12)
R2_is_bad <- function( quap_fit ) {
  s <- sim( quap_fit , refresh=0 ) 
  r <- apply(s,2,mean) - d$brain_std 
  1 - var2(r)/var2(d$brain_std)
}
```

```{r}
R2_is_bad(m7.1)
```

Five more models in increasing complexity - just a polynomial of higher degree.

Body to brain size is a parabola:

\begin{align*}
  b_i &\sim \text{Normal}(\mu_i, \sigma)\\
  \mu_i &= \alpha + \beta m_i + \beta_2 m_i^2\\
  \alpha &\sim \text{Normal}(0.5,1)\\
  \beta_j &\sim \text{Normal}(0,10) \text{    for }j=1..2\\
  \sigma &\sim \text{Log-Normal}(0,1)
\end{align*}

```{r}
m7.2 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ), 
    mu <- a + b[1]*mass_std + b[2]*mass_std^2,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ), 
    log_sigma ~ dnorm( 0 , 1 )
), data=d , start=list(b=rep(0,2)) )
```

Continue and make third, fourth, fifth degree models (code hidden from pdf, since it's the same thing over and over

```{r}
m7.3 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ), 
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 +b[3]*mass_std^3, 
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
  ), data=d , start=list(b=rep(0,3)) 
)

m7.4 <- quap( 
  alist(
      brain_std ~ dnorm( mu , exp(log_sigma) ), 
      mu <- a + b[1]*mass_std + b[2]*mass_std^2 +b[3]*mass_std^3 + b[4]*mass_std^4, 
      a ~ dnorm( 0.5 , 1 ),
      b ~ dnorm( 0 , 10 ),
      log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,4)) 
)

m7.5 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ), 
    log_sigma ~ dnorm( 0 , 1 )
  ), data=d , start=list(b=rep(0,5)) 
)
```


For the last model, sixth order, we have to replace std with 0.001 (will explain later)

```{r}
m7.6 <- quap( 
  alist(
    brain_std ~ dnorm( mu , 0.001 ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 + 
              b[3]*mass_std^3 + b[4]*mass_std^4 +
              b[5]*mass_std^5 + b[6]*mass_std^6, 
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 )
    ), data=d , start=list(b=rep(0,6))
  )
```

Plotting:

```{r}
post <- extract.samples(m7.1)
mass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 ) 
l <- link( m7.1 , data=list( mass_std=mass_seq ) )
mu <- apply( l , 2 , mean )
ci <- apply( l , 2 , PI )
plot( brain_std ~ mass_std , data=d )
lines( mass_seq , mu )
shade( ci , mass_seq )
```

```{r}
post <- extract.samples(m7.6)
mass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 ) 
l <- link( m7.6 , data=list( mass_std=mass_seq ) )
mu <- apply( l , 2 , mean )
ci <- apply( l , 2 , PI )
plot( brain_std ~ mass_std , data=d )
lines( mass_seq , mu )
shade( ci , mass_seq )
```
```{r}
R2_is_bad(m7.6)
```

$R^2$ allegedly perfect! But that's because the degrees of freedom equal the number of data points. Clearly won't work for future cases.


### 7.1.2 Too few parameters hurts, too

Overfitting is accurate in-sample, but inaccurate out-of-sample. Underfitting is inaccurate both in and out of sample. One check is removing a data point - overfitting mean changes a lot, underfitting doesn't.

Rethinking - Bias and Variance: Often under/overfitting is described as _bias-variance tradeoff_, where "bias" is underfitting, "variance" is overfitting.

## 7.2 Entropy and Accuracy

### Firing the weatherperson

Accuracy depends on target, no best target. To consider:

1) Cost-benefit analysis. Both cost for incorrect, and gain for correct

2) Accuracy in context

Consider rain example - cost of not bringing an umbrella is much worse than bringing one and not using it.

Another approach is a joint probability. If you use Bayes to estimate the chance of getting 10 days right, the model based one has a higher chance (0.005%) over the one predicting it won't ever rain (0%), even if it has a higher hit rate. Joint probability is great because it's the likelihood in Bayes theorem.

Called _log scoring rule_ sometimes, since typically the log of the joint probability is reported.

### Information and uncertainty

How to measure from perfect prediction? Distance to target, but if you add snow, then what? 

Field of _information theory_ developed to address this.

_Information_: Reduction in uncertainty when we learn the true outcome

Weather example - each has a probability, want to turn to uncertainties. _Information Entropy_: for $n$ events, each event $i$ has probability $p_i$, then unique uncertainty is:

$$
H(p) = -E\log (p_i) = -\sum_{i=1}^{n} p_i \log (p_i)
$$

Or, "uncertainty contained in a probability distribution is the average log-probability of an event"

Working example - true probabilities for rain and shine are $p_1=0.3$, $p_2=0.7$. $H(p) = -(p_1)\log(p_1)+ p_2\log(p_2)$

```{r}
p<- c(0.3, 0.7)
-sum(p*log(p))
```

In a flip calculation, a place where it rains very little might be 0.01 and 0.99, making entropy 0.06. Much less uncertainty.

### From entropy to accuracy 

Entropy tells us how hard to hit the target. Now want to know how far a model is from the target

Kullback-Leibler _Divergence_ - Additional uncertainty induced by using probabilities from one distribution to describe another.

$$
  D_{KL}(p,q) = \sum_i p_i (\log(p_i)-\log(q_i)) = \sum_i \log(\frac{p_i}{q_i})
$$

For using model assigning probabilities $q_i$ to an event with true probabilities $p_i$. _Divergence is the average difference in log probability between the target $p$ and the model $q$. This is the difference between two entropies - entropy of target $p$ and cross entropy of using $q$ to predict $p$.

This can help contrast different approximations to $p$, candidate that minimizes divergence is closest to target.

### Estimating Divergence

What if we don't know $p$ though? 

Comparing two models ($q$ and $r$), this is easy to get out of - $p$ subtracts out. We compare average log-probability from each model to get an estimate of relative distance from each to the target. In this, magnitudes are worthless, but difference is useful. For model $q$:

$$
S(q) = \sum_i \log(q_i)
$$

For a Bayesian model, you must compute for the entire posterior though (parameters have distributions, ergo predictions do too). Find log of average probability for each observation; ```lppd``` function in ```rethinking``` does this:

```{r}
set.seed(1)
lppd( m7.1 , n=1e4 )
```

Sum for log probability score, maximum is better. Multiply by -2 for _deviance_, smaller is better.

### Scoring the right data

Maintain same problem - always get better for increasingly complex models:

```{r}
set.seed(1)
sapply( list(m7.1,m7.2,m7.3,m7.4,m7.5,m7.6) , function(m) sum(lppd(m)) )
```

What interests us is score on _new_ data, so do a _train-test split_.


## 7.3 Golem Taming: Regularization

One way to make a model overtrain less is to use a skeptical prior, most commonly _regularizing prior_. Consider a standard Gaussian - prior on 

\begin{align*}
  y_i &\sim \text{Normal}(\mu_i, \sigma)\\
  \mu_i &= \alpha + \beta x_i\\
  \alpha &\sim \text{Normal}(0, 100)\\
  \beta &\sim \text{Normal}(0,1)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

$\beta$ here says we should be skeptical of values above 2 and below -2 (contain 5% of plausibility). Can make it increasingly skeptical by narrowing. Careful though, can prevent learning from data if too tight.

## 7.4 Predicting Predictive Accuracy

### Cross-Validation

Popular way to estimate predictive accuracy is to test on small parts of sample, known as _cross-validation_. Usually done in folding

How many folds? Common to use maximum number, leave out one unique observation per fold: _Leave-One-Out Cross-Validation_ (LOOCV). 

Great for small sets, but not so great for large. One option is to estimate the _importance_ of each sample, an unlikely observation is more important that a likely one. Often also called a _weight_. Method of cross validation is called _Pareto-Smoothed Importance Sampling Cross-Validation_ (PSIS). PSIS uses the _Pareto distribution_ to derive a cross-validation score without actually doing cross validation.

Difference between points in training and testing deviance is about twice the number of model parameters; for OLS with flat priors this is the expectation. Used in _information criteria_, providing an estimate of out-of-sample deviance:

$$
\text{AIC} = D_{\text{train}} + 2p = -2\text{lppd}+2p
$$
Where AIC refers to the _Akaike Information Criterion_, and lppd is the log-posterior-predictive-density Mainly of historical interest, only reliable when priors are flat, posterior is Gaussian, and samples is much greater than number of parameters.

_Deviance Information Criterion_ - okay with informative priors but maintains other two assumptions.

_Widely Applicable Information Criterion_ - More general, no assumption on posterior shape.

$$
\text{WAIC}(y,\Theta) = -2(\text{lppd} - \sum_i \text{var}_{\theta}\log p(y_i|\theta) )
$$

The last term just being a penalty proportional to the variance -"compute variance in log-probabilities for each observation, then sum to get total penalty." Sometimes called _effective number of parameters_, $p_\text{WAIC}$ (historical, not accurate).

Provides an approximate estimate of standard error for out-of-sample deviance, using central limit theorem.

$$
s_\text{WAIC} = \sqrt{N \text{var} - 2 (\text{lppd}_i - p_i)}
$$

Rethinking - Bayes Factors: _Bayesian Information Criterion_ commonly juxtaposed with AIC. BIC is related to log of the average likelihood. Tradition to take the ratio of likelihoods as a _Bayes  Factor_.

### Comparing CV, PSIS, WAIC

Do a simulation exercise, look at all three. For a low $N=20$, all are close, but miss by some amount. For $N=100$, all 3 become identical. 

## 7.5 Model Comparison

_Model selection_ - choosing model with lowest criterion and discarding others. Don't do this, it discards information about relative model accuracy contained in CV/PSIS/WAIC values. Also, predictive accuracy is not causation.

Instead, _model comparison_ - using multiple models to understand how different variables influence predictions .

### Model Mis-Selection

Go back to models 6.6-6.8, looking at fungus. 6.6 has just the intercept, 6.7 has treatment and fungus (post-treatment), and 6.8 allows causal influence of treatment

```{r, include=FALSE}
set.seed(71) 
# number of plants 
N <- 100
# simulate initial heights 
h0 <- rnorm(N,10,2)
# assign treatments and simulate fungus and growth 
treatment <- rep( 0:1 , each=N/2 )
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 ) 
h1 <- h0 + rnorm(N, 5 - 3*fungus)
# compose a clean data frame
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus ) 

m6.6 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ), 
    mu <- h0*p,
    p ~ dlnorm( 0 , 0.25 ), 
    sigma ~ dexp( 1 )
), data=d ) 
precis(m6.6)

m6.7 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment + bf*fungus, 
    a ~ dlnorm( 0 , 0.2 ) ,
    bt ~ dnorm( 0 , 0.5 ),
    bf ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.7)

m6.8 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment,
    a ~ dlnorm( 0 , 0.2 ), 
    bt ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
  ), data=d ) 

```

```{r}
set.seed(11) 
WAIC( m6.7 )
```

WAIC is out-of-sample deviance, second is lppd, third is effective number of parameters, and fourth is standard error of WAIC.

```{r}
set.seed(77)
compare( m6.6 , m6.7 , m6.8 )
```

pWAIC = penalty term, dWAIC is difference in WAIC to best, SE is standard error of each WAIC, dSE is standard error of their difference:
```{r}
set.seed(91)
waic_m6.7 <- WAIC( m6.7 , pointwise=TRUE )$WAIC 
waic_m6.8 <- WAIC( m6.8 , pointwise=TRUE )$WAIC 
n <- length(waic_m6.7)
diff_m6.7_m6.8 <- waic_m6.7 - waic_m6.8
sqrt( n*var( diff_m6.7_m6.8 ) )
```

so difference between models is about 41, standard error is about 10.4.

```{r}
 plot( compare( m6.6 , m6.7 , m6.8 ) )
```

m6.7 looks the best - filled in points are in-sample, empty are out-of-sample. Lighter line is standard error of the difference in WAIC between models.

WAIC tells us which model predicts better here, not which tells us the most about causation.

WAIC also finds models 6.6 (intercept model) and 6.8 (treatment-only) similar.

```{r}
set.seed(92)
waic_m6.6 <- WAIC( m6.6 , pointwise=TRUE )$WAIC 
diff_m6.6_m6.8 <- waic_m6.6 - waic_m6.8
sqrt( n*var( diff_m6.6_m6.8 ) )
```

Less than 5 standard error of the difference. Also, this is larger than the difference itself: cannot distinguish on WAIC.

Last thing in the compare table is _weight_, how to summarize support for a model (sum to 1).

$$
w_i = \frac{\exp(-0.5 \Delta_i)}{\sum_j \exp (-0.5\Delta_j)}
$$

Quick way to see differences among models, also used in _model averaging_.

### Outliers and other illusions

Waffle house divorce rate example, had outliers, consider models from chapter 5:

```{r}
library(rethinking) 
data(WaffleDivorce) 
d <- WaffleDivorce 
d$A <- standardize( d$MedianAgeMarriage )
d$D <- standardize( d$Divorce )
d$M <- standardize(d$Marriage )

m5.1 <- quap( 
  alist(
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bA * A ,
    a ~ dnorm( 0 , 0.2 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = d )

m5.2 <- quap( 
  alist(
    D ~ dnorm( mu , sigma ) ,
    mu <- a + bM * M ,
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = d )    

m5.3 <- quap( 
  alist(
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bM*M + bA*A , 
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = d )
```

Compare with PSIS:

```{r}
set.seed(24071847)
compare( m5.1 , m5.2 , m5.3 , func=PSIS )
```

Model without marriage rate is best. Error messages shows PSIS isn't reliable
```{r}
set.seed(24071847)
PSIS_m5.3 <- PSIS(m5.3,pointwise=TRUE)
set.seed(24071847)
WAIC_m5.3 <- WAIC(m5.3,pointwise=TRUE)
plot( PSIS_m5.3$k , WAIC_m5.3$penalty , xlab="PSIS Pareto k" ,
    ylab="WAIC penalty" , col=rangi2 , lwd=2 )
```


Idaho's high PSIS Pareto k value and WAIC penalty (effective number of parameters) is clear here. Pareto k value is double the theoretical point at which variance becomes infinite.

What to do here? Gaussians have thin tails, but many natural phenomena don't.

_Robust Regression_ - linear model where extreme observations are reduced. 

A common kind is the _Student's T_ distribution, a mixture of Gaussian distributions with different variances - has a third parameter $\nu$, controlling tail thickness. For large datasets you can estimate $\nu$, but usually just assume it's small to reduce the influence of outliers.


```{r}
m5.3t <- quap( 
  alist(
    D ~ dstudent( 2 , mu , sigma ) , 
    mu <- a + bM*M + bA*A ,
    a ~ dnorm( 0 , 0.2 ) ,
    bM ~ dnorm( 0 , 0.5 ) ,
    bA ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 ) ) , 
data = d )
PSIS(m5.3t)
```

No warnings about k values now, using student distribution with $\nu=2$.

## 7.6 Summary

- Overfitting - too many parameters is bad
- Regularizing priors and out-of-sample accuracy estimates like WAIC and PSIS to navigate


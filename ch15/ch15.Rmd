# Chapter 15 - Missing Data and Other Opportunities
```{r, include=FALSE}
library(rethinking)
library(dagitty)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

An advantage of Bayesian inference is that it gets rid of the need to be clever. Burnt pancake example of conditional probability - think of ways you could be looking at the data (count _sides_ of pancakes), rather than outcome-based (which pancake?). We can also think of BDA as:

$$
  \Pr (\text{want to know}|\text{already know})
$$

Probability theory is hard to interpret and apply, often requiring cleverness, but Bayesian approach applies conditional probability - once we define assumptions, probability just does the rest.

This chapter shows two applications of assume-and-deduce strategy - incorporating _measurement error_ and then estimation of missing data through _Bayesian Imputation_. 

## 15.1 - Measurement Error 

Using divorce data. Both divorce variable and marriage rate have standard errors provided

```{r}
data("WaffleDivorce")
d<-WaffleDivorce
head(d)
```

Shown in the .SE columns. Plotting this,

```{r}
# points
plot( d$Divorce ~ d$MedianAgeMarriage , ylim=c(4,15) ,
    xlab="Median age marriage" , ylab="Divorce rate" )

# standard errors
for ( i in 1:nrow(d) ) {
    ci <- d$Divorce[i] + c(-1,1)*d$Divorce.SE[i]
    x <- d$MedianAgeMarriage[i]
    lines( c(x,x) , ci )
}
```


Error varies due to state size. Some states are more confident, so we should weight those higher.

### Error on the Outcome

Think of how we'd generate data, what would we want errors to look like? Decrease as log population increases.

A DAG:

```{r}
dag <- dagitty("dag{ 
  A-> M -> D
  A-> D -> D_obs <- e_D
  D [unobserved]
  e_D [unobserved]
  }")
coordinates(dag) <- list( x=c(A=0,M=1,D=1,D_obs=2,e_D=3) , y=c(A=1,M=0,D=2,D_obs=2,e_D=2) )
drawdag(dag)
```


True divorce rate cannot be observed, but we do have a observed divorce rate that's a function of the true divorce rate and the error on it. Most regressions in general are really looking at $D_{obs}$ as a proxy for $D$.

How to put into a model - like a simulation but in reverse, assign a distribution to observations and draw from it. For examplue, if a measurement is 10 meters with standard Guassian deviation of 2, implies 

$$
  y \sim \text{Normal}(10,2)
$$

The key insight - if we don't know the true value, we can put a parameter and let Bayes do the rest.

Going back to divorce rate, we can use the parameter:

$$
  D_{obs,i} \sim \text{Normal}(D_{true,i},D_{SE,i})
$$

Full model:

\begin{align*}
  D_{obs,i} &\sim \text{Normal}(D_{true,i},D_{SE,i})\\
  D_{true,i} &\sim \text{Normal}(\mu_i, \sigma)\\
  \mu_i &= \alpha + \beta_A A_i + \beta_M M_i \\
  \alpha &\sim \text{Normal}(0,0.2)\\
  \beta_A &\sim \text{Normal}(0,0.5)\\
  \beta_M &\sim \text{Normal}(0, 0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

Effectively just a linear regression, but you have a connecting term from observation to true parameter value.

```{r, results=FALSE, message=FALSE, warning=FALSE}
dlist <- list(
    D_obs = standardize( d$Divorce ),
    D_sd = d$Divorce.SE / sd( d$Divorce ),
    M = standardize( d$Marriage ),
    A = standardize( d$MedianAgeMarriage ),
    N = nrow(d)
)

m15.1 <- ulam(
    alist(
        D_obs ~ dnorm( D_true , D_sd ),
        vector[N]:D_true ~ dnorm( mu , sigma ),
        mu <- a + bA*A + bM*M,
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=dlist , chains=4 , cores=4 , cmdstan=TRUE)
```

```{r}
precis(m15.1)
```

If we consider the posterior means, previously ```bA``` was -1, now it's about half that, but still reliably negative, the error reduced the association - not always the case.

If you plot the previous vs new model, one thing to notice is that the less certain estimates are more susceptible to shrinkage than the more certain ones.

### Error on Both the Outcome and Predictor

If there's error on predictors, the approach is the same.

```{r}
dag <- dagitty("dag{ 
  A-> M -> D
  A-> D -> D_obs <- e_D
  M -> M_obs <- e_M
  M [unobserved]
  e_M [unobserved]
  D [unobserved]
  e_D [unobserved]
  }")
coordinates(dag) <- list( x=c(A=0,M=1,D=1,D_obs=2,e_D=3,M_obs=2,e_M=3) , y=c(A=1,M=0,D=2,D_obs=2,e_D=2,M_obs=0,e_M=0) )
drawdag(dag)
```

With model:

\begin{align*}
  D_{obs,i} &\sim \text{Normal}(D_{true,i},D_{SE,i})\\
  D_{true,i} &\sim \text{Normal}(\mu_i, \sigma)\\
  \mu_i &= \alpha + \beta_A A_i + \beta_M M_{true,i} \\
  M_{obs,i} &\sim \text{Normal}(M_{true,i}, M_{SE,i})\\
  M_{true,i} &\sim \text{Normal}(0,1)\\
  \alpha &\sim \text{Normal}(0,0.2)\\
  \beta_A &\sim \text{Normal}(0,0.5)\\
  \beta_M &\sim \text{Normal}(0, 0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}


```{r, results=FALSE, message=FALSE, warning=FALSE}
dlist <- list(
    D_obs = standardize( d$Divorce ),
    D_sd = d$Divorce.SE / sd( d$Divorce ),
    M_obs = standardize( d$Marriage ),
    M_sd = d$Marriage.SE / sd( d$Marriage ),
    A = standardize( d$MedianAgeMarriage ),
    N = nrow(d)
)

m15.2 <- ulam(
    alist(
        D_obs ~ dnorm( D_true , D_sd ),
        vector[N]:D_true ~ dnorm( mu , sigma ),
        mu <- a + bA*A + bM*M_true[i],
        M_obs ~ dnorm( M_true , M_sd ),
        vector[N]:M_true ~ dnorm( 0 , 1 ),
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp( 1 )
    ) , data=dlist , chains=4 , cores=4 , cmdstan = TRUE)
```

```{r}
precis(m15.2)
```

Output is pretty similar, but we did get updated estimates of marriage rate.

```{r}
post <- extract.samples( m15.2 )
D_true <- apply( post$D_true , 2 , mean )
M_true <- apply( post$M_true , 2 , mean )
plot( dlist$M_obs , dlist$D_obs , pch=16 , col=rangi2 ,
    xlab="marriage rate (std)" , ylab="divorce rate (std)" )
points( M_true , D_true )
for ( i in 1:nrow(d) )
    lines( c( dlist$M_obs[i] , M_true[i] ) , c( dlist$D_obs[i] , D_true[i] ) )
```


The big takeaway - if you have a big distribution of values, don't use a single value for regression, use the distribution.

### Measurement Terrors

Errors can be difficult to manage, especialy if correlated. Imagine for some variable $P$,

```{r}
dag <- dagitty("dag{ 
  A-> M -> D
  A-> D -> D_obs <- e_D
  M -> M_obs <- e_M
  e_M <- P -> e_D
  M [unobserved]
  e_M [unobserved]
  D [unobserved]
  e_D [unobserved]
  }")
coordinates(dag) <- list( x=c(A=0,M=1,D=1,D_obs=2,e_D=3,M_obs=2,e_M=3,P=4) , y=c(A=1,M=0,D=2,D_obs=2,e_D=2,M_obs=0,e_M=0, P=1) )
drawdag(dag)
```


If we regress $D_{obs}$ on $M_{obs}$, there's an open, non-causal path. A possible solution is to model true $D$ and $M$, if we know the measurement process.

There's also the case where one variable influences the error and creates a non-causal path:
```{r}
dag <- dagitty("dag{ 
  A-> M -> D
  A-> D -> D_obs <- e_D
  e_D <- M -> M_obs <- e_M
  M [unobserved]
  e_M [unobserved]
  D [unobserved]
  e_D [unobserved]
  }")
coordinates(dag) <- list( x=c(A=0,M=1,D=1,D_obs=2,e_D=3,M_obs=2,e_M=3) , y=c(A=1,M=0,D=2,D_obs=2,e_D=2,M_obs=0,e_M=0) )
drawdag(dag)
```

This might happen if marriages are rare, then fewer divorce possibilities, then smaller sample to measure divorce, so ultimately small $M$ induces large $e_D$.

Anther problem is when a causal variable is measured less precisely than a non-causal. Say we know $D$ and $M$ well but $A$ has a lot of error, and no causality between $M$ and $D$:

```{r}
dag <- dagitty("dag{ 
  e_A [unobserved]
  A [unobserved]
  e_A -> A_obs <- A
  M <- A -> D
  }")
coordinates(dag) <- list( x=c(e_A=0, A_obs=1, A=2, D=3, M=3) , y=c(e_A=1, A_obs=1, A=1, D=2, M=0) )
drawdag(dag)
```

Here, a naive regression of $D$ on $A_{obs}$ and $M$ will suggest $M$ influences $D$ - $M$ basically functions as proxy $A$.

## 15.2 - Missing Data

What to do if data is missing?

Common behavior is to just drop all cases - _Complete Case Analysis_. It wastes data though.

Instead _impute_ missing data. Generative models tell you weather the process that produced missing values will also prevent the identification of causal effects. You can add missing contributions to a DAG too.

Rethinking - Missing data are meaningful: missing values depend on context. If someone omits income on a form, often this may mean a particularly low or high value. In ecology, if you haven't observed a species it might mean it's not there, it also might mean you just haven't seen one. _Occupancy models_ try to take this into account.

### DAG ate my homework

Sample of students who own dogs - produce homework $H_p$, influenced by how much they study $S$. Dogs eat some of the homework ($D$), so we can't see the true distribution, only those returned $H_m$. Can think of several causal scenarios:

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
  S -> H_p -> H_m <-D
  H_p [unobserved]
  }")
coordinates(dag) <- list( x=c(S=0, D=0, H_p=1, H_m=1) , y=c(S=0, D=1, H_p=0, H_m=1) )
drawdag(dag)
```

Simplest shown - completely random.

```{r}
# Simulate 100 students with attributes and homework
N <- 100
S <- rnorm( N )
H <- rbinom( N , size=10 , inv_logit(S) )

# Let dogs randomly eat homework sets
D <- rbern( N ) # dogs completely random
Hm <- H # All homework
Hm[D==1] <- NA # Dropout those eaten as NANs
```

Are these missing values specifically problematic? Depends - is outcome $H$ independent of $D$? If so, then the missing values don't change the overall distribution in a systematic way. Estimation is less efficient, but doesn't induce a bias.

But what if $S$ influences $D$? More studying causes dogs to want to eat homework.

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
  S -> H_p -> H_m <-D
  S-> D
  H_p [unobserved]
  }")
coordinates(dag) <- list( x=c(S=0, D=0, H_p=1, H_m=1) , y=c(S=0, D=1, H_p=0, H_m=1) )
drawdag(dag)
```

This scenario is far more problematic, students that study more than average have their homeworks eaten more. Backdoor non-causal path from $H_p \rightarrow H_m \leftarrow D \leftarrow S$. If left unclosed, confound inference along $S\rightarrow H_p$, so just need to condition on $S$ (we're doing that anyway). Still need to keep in mind the danger if we get functions or distributions wrong.

Next scenario:

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
  S -> H_p -> H_m <-D
  S-> D
  D<-X->H_p
  H_p [unobserved]
  X [unobserved]
  }")
coordinates(dag) <- list( x=c(S=0, D=0, H_p=1, H_m=1,X=0.5) , y=c(S=0, D=1, H_p=0, H_m=1,X=0.5) )
drawdag(dag)
```

New variable $X$ represents how noisy a house is - worse homework $X \rightarrow H_p$, and also more misbehaving dog $X\rightarrow D$. New causal path $H_m \leftarrow D \leftarrow X \rightarrow H_p$.

Simulating this and building a binomial model

```{r, results=FALSE, message=FALSE, warning=FALSE}
set.seed(501)
N <- 1000
X <- rnorm(N)
S <- rnorm(N)
H <- rbinom( N , size=10 , inv_logit( 2 + S - 2*X ) )
D <- ifelse( X > 1 , 1 , 0 )
Hm <- H
Hm[D==1] <- NA

dat_list <- list(
    H = H,
    S = S )

m15.3 <- ulam(
    alist(
        H ~ binomial( 10 , p ),
        logit(p) <- a + bS*S,
        a ~ normal( 0 , 1 ),
        bS ~ normal( 0 , 0.5 )
    ), data=dat_list , chains=2 ,cmdstan = TRUE)
```

```{r}
precis(m15.3)
```

$S$ coefficient should be 1.00, but isn't - _omitted variable bias_.

Also look at the impact of missing data - same model, but use $H_p$ instead of $H_m$ for cases where $D=1$.

```{r,  results=FALSE, message=FALSE, warning=FALSE}
dat_list0 <- list( H = H[D==0] , S = S[D==0] )

m15.4 <- ulam(
    alist(
        H ~ binomial( 10 , p ),
        logit(p) <- a + bS*S,
        a ~ normal( 0 , 1 ),
        bS ~ normal( 0 , 0.5 )
    ), data=dat_list0 , chains=2, cmdstan = TRUE)
precis( m15.4 )
```

```{r}
precis(m15.4)
```


Coefficient is less biased, but still biased. In fact, deleting noisy houses from the data makes the estimate better.

Last case:

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
  S -> H_p -> H_m <-D
  S-> D
  H_p -> D
  H_p [unobserved]
  }")
coordinates(dag) <- list( x=c(S=0, D=0, H_p=1, H_m=1) , y=c(S=0, D=1, H_p=0, H_m=1) )
drawdag(dag)
```

Path from $H_p \rightarrow D$, dogs prefer to eat bad homework. This is the worst kind of missing value - variable causes its own missing values. Nothing we can do to condition on the non-causal path $S \rightarrow H_p \rightarrow D \rightarrow H_m$. Unless you know the mechanism that produces missing data, there's little hope - measurement is all we have.

### Imputing Primates

Can _impute_ missing values to avoid biased estimation and use all observed data.

Go back to milk/neocortex size example, 12 missing values on ```neocortex.perc``` column. Before, we dropped those (_complete-case_), but that's 12/29 data points.

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
               K<-M<-U->B->K
               U [unobserved]
               }")
coordinates(dag) <- list( x=c(M=0, U=1, K=1, B=2) , y=c(M=0, U=0, K=1, B=0) )
drawdag(dag)
```

$U$ makes $M$ and $B$ correlated. The observed values $B_m$ are partially observed, so adding missingness to this DAG, can occur a few ways. Say the process which causes missingness generates $R_B$ which indicates which species have missing values.

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
               K<-M<-U->B->K
               B-> B_m <- R_B
               U [unobserved]
               B [unobserved]
               }")
coordinates(dag) <- list( x=c(M=0, U=1, K=1, B=2, B_m=2, R_B=1) , y=c(M=0, U=0, K=1, B=0,B_m=-1, R_B=-1) )
drawdag(dag)
```

Case here, $R_B$ is completely random - no new non-causal paths. This doesn't cause any problems with our earlier approach, since dropping the values doesn't bias the inference.

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
               K<-M<-U->B->K
               B-> B_m <- R_B <- M
               U [unobserved]
               B [unobserved]
               }")
coordinates(dag) <- list( x=c(M=0, U=1, K=1, B=2, B_m=2, R_B=1) , y=c(M=0, U=0, K=1, B=0,B_m=-1, R_B=-1) )
drawdag(dag)
```

If the missing data is influenced by body mass then we have a new non-causal path, $B_m \leftarrow R_B \leftarrow M \rightarrow K$. But since we're conditioning on $M$, this blocks the path. This might happen if smaller primates are studied more than larger ones. Still want to impute values so we don't throw away information.

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
               K<-M<-U->B->K
               R_B <- B-> B_m <- R_B
               U [unobserved]
               B [unobserved]
               }")
coordinates(dag) <- list( x=c(M=0, U=1, K=1, B=2, B_m=2, R_B=1) , y=c(M=0, U=0, K=1, B=0,B_m=-1, R_B=-1) )
drawdag(dag)
```

This case, brain size influences $R_B$, perhaps if scientists are more interested in large brained species. This scenario is the worst, $B \rightarrow K$ is biased by a non-causal path through $R_B$ and not able to test if $B$ influences $R_B$.

These arise in many ways, consider:

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
               K<-M<-U->B->K
               V->B->B_m<-R_B<-V
               U [unobserved]
               B [unobserved]
               V [unobserved]
               }")
coordinates(dag) <- list( x=c(M=0, U=1, K=1, B=2, B_m=3, R_B=3, V=2.5) , y=c(M=0, U=0, K=1, B=0,B_m=0, R_B=1, V=0.5) )
drawdag(dag)
```

Some unobserved $V$ influences both $B$ and $R_B$, could be something like similarity to humans, which motivates more study.


So what to do? In all cases, we'd like to impute missing values of $B$. case 1 and 2, do so in order to not throw away corresponding $M$ values. In the third, we have to impute for a sensible $B\rightarrow K$ estimate.

How to impute? Model the variable that has missing values, give missing values unique parameters, then let that distribution become a prior for missing values. Ultimately we get a posterior for missing values. Say for $B$,

$$ 
  B= [0.55, B_2, B_3,B_4,0.65,0.65,\dots,0.76,0.75]
$$

We give parameter $B_i$ for missing values.

\begin{align*}
  K_i &\sim \text{Normal}(\mu_i,\sigma)
  \mu_i &= \alpha + \beta_B B_i + \beta_M \log M_i \\
  B_i &\sim \text{Normal}(\nu, \sigma_B)\\
  \alpha &\sim \text{Normal}(0,0.5)\\
  \beta_B &\sim \text{Normal}(0,0.5)\\
  \beta_M &\sim \text{Normal}(0,0.5)\\
  \sigma &\sim \text{Exponential}(1)\\
  \nu &\sim \text{Normal}(0.5,1)\\
  \sigma_B &\sim \text{Exponential}(1)\\
\end{align*}

We have $B_i$ as the distribution for missing $B$, when its a value, this is a likelihood - $\nu$ and $\sigma_B$ are learned from data. When it's missing, the line is interpreted as a prior. This is a little awkward because we know values are bounded on [0,1].

Imputing can be awkward, because locations lead to index management. Rethinking ```ulam``` automates.

```{r,results=FALSE, message=FALSE, warning=FALSE}
library(rethinking)
data(milk)
d <- milk
d$neocortex.prop <- d$neocortex.perc / 100
d$logmass <- log(d$mass)
dat_list <- list(
    K = standardize( d$kcal.per.g ),
    B = standardize( d$neocortex.prop ),
    M = standardize( d$logmass ) )

m15.5 <- ulam(
    alist(
        K ~ dnorm( mu , sigma ),
        mu <- a + bB*B + bM*M,
        B ~ dnorm( nu , sigma_B ),
        c(a,nu) ~ dnorm( 0 , 0.5 ),
        c(bB,bM) ~ dnorm( 0, 0.5 ),
        sigma_B ~ dexp( 1 ),
        sigma ~ dexp( 1 )
    ) , data=dat_list , chains=2 , cores=2,cmdstan = TRUE)
```

```{r}
precis( m15.5 , depth=2 )
```


Notice precis gives a unique distribution for missing values.

Also will build a corresponding non-imputed model and compare.

```{r,results=FALSE, message=FALSE, warning=FALSE}
obs_idx <- which( !is.na(d$neocortex.prop) )
dat_list_obs <- list(
    K = dat_list$K[obs_idx],
    B = dat_list$B[obs_idx],
    M = dat_list$M[obs_idx] )
m15.6 <- ulam(
    alist(
        K ~ dnorm( mu , sigma ),
        mu <- a + bB*B + bM*M,
        B ~ dnorm( nu , sigma_B ),
        c(a,nu) ~ dnorm( 0 , 0.5 ),
        c(bB,bM) ~ dnorm( 0, 0.5 ),
        sigma_B ~ dexp( 1 ),
        sigma ~ dexp( 1 )
    ) , data=dat_list_obs , chains=2 , cores=2 ,cmdstan = TRUE)
```

```{r}
precis( m15.6 )
```

Looking at posteriors:
```{r}
plot( coeftab(m15.5,m15.6) , pars=c("bB","bM") )
```

Top is bB, bottom is bM. Notice imputed model has narrower distributions for these, since it uses more information.

```{r}
post <- extract.samples( m15.5 )
B_impute_mu <- apply( post$B_impute , 2 , mean )
B_impute_ci <- apply( post$B_impute , 2 , PI )

# B vs K
plot( dat_list$B , dat_list$K , pch=16 , col=rangi2 ,
    xlab="neocortex percent (std)" , ylab="kcal milk (std)" )
miss_idx <- which( is.na(dat_list$B) )
Ki <- dat_list$K[miss_idx]
points( B_impute_mu , Ki )
for ( i in 1:12 ) lines( B_impute_ci[,i] , rep(Ki[i],2) )

# M vs B
plot( dat_list$M , dat_list$B , pch=16 , col=rangi2 ,
    ylab="neocortex percent (std)" , xlab="log body mass (std)" )
Mi <- dat_list$M[miss_idx]
points( Mi , B_impute_mu )
for ( i in 1:12 ) lines( rep(Mi[i],2) , B_impute_ci[,i] )

```

Imputed values are shown in blue with 89% compaitbility intervals in the line segments - of course there's a lot of uncertainty.
  


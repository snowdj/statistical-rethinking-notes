# Chapter 6 - The Haunted DAG & The Causal Terror
```{r, include=FALSE}
library(rethinking)
```

Take two uncorrelated variables (newsworthiness and trustworthiness), select the top 10%, and draw a linear regression and there'll be a negative correlation - _Berkson's Paradox_ or _selection-distortion_ effect.

This can happen within multiple regression, if adding a predictor induces selection, known as _collider bias_.

In this chapter - warnings about adding random variables without an idea of a causal model: multicollinearity, post-treatment bias, collider bias.

## 6.1 Multicollinearity

"Very strong correlation between two or more predictor variables"

Technically nothing wrong with it, will work well for prediction, just hard to understand.

Return to primate milk data for data.


### Multicollinear legs

Using both leg lengths (left and right) as a predictor

```{r}
# Generate dataframe
N <- 100
set.seed(909)
height <- rnorm(N,10,2) 
leg_prop <- runif(N,0.4,0.5) 
leg_left <- leg_prop*height +
  rnorm( N , 0 , 0.02 ) 
leg_right <- leg_prop*height +
  rnorm( N , 0 , 0.02 )
d <- data.frame(height,leg_left,leg_right)

# Make model
m6.1 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + bl*leg_left + br*leg_right , 
    a ~ dnorm( 10 , 100 ) ,
    bl ~ dnorm( 2 , 10 ) ,
    br ~ dnorm( 2 , 10 ) ,
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.1)
```

```{r}
plot(precis(m6.1))
```

Gigantic means and standard deviations. Question posed in last chapter is "what's the value of knowing a second leg length after knoing the first?" 

```{r}
post <- extract.samples(m6.1)
plot( bl ~ br , post , col=col.alpha(rangi2,0.1) , pch=16 )
```

Posterior for left vs right very highly correlated - as $\beta_l$ is large, $\beta_r$ is small, they carry the same variation.

Our model development:

\begin{align*}
  y_i &\sim \text{Normal}(\mu,\sigma)\\
  \mu_i &= \alpha + \beta_1 x_l + \beta_2 x_r\\
\end{align*}

But due to covariation (x is basically the samething), the computer basically sees:

\begin{align*}
  y_i &\sim \text{Normal}(\mu,\sigma)\\
  \mu_i &= \alpha + (\beta_1 +\beta_2)  x_i\\
\end{align*}

As in only the sum of $\beta_1 + \beta_2$ influences $\mu$.

```{r}
sum_blbr <- post$bl + post$br
dens( sum_blbr , col=rangi2 , lwd=2 , xlab="sum of bl and br" )
```


Using just one predictor:

```{r}
m6.2 <- quap( 
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + bl*leg_left,
    a ~ dnorm( 10 , 100 ) ,
    bl ~ dnorm( 2 , 10 ) ,
    sigma ~ dexp( 1 ) 
  ),
  data=d ) 
precis(m6.2)
```

Which is basically identical to the other model - when the two predictor variables are strongly carrelated, including both may lead to confusion.

### Multicollinear milk

Legs example is contrived, something more elaborate

```{r}
data(milk)
d <- milk
d$K <- scale( d$kcal.per.g ) 
d$F <- scale( d$perc.fat ) 
d$L <- scale( d$perc.lactose )
```

Looking at bivariate regressions:


```{r}
# kcal.per.g regressed on perc.fat 
m6.3 <- quap(
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bF*F ,
    a ~ dnorm( 0 , 0.2 ) , 
    bF ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ) , data=d )

# kcal.per.g regressed on perc.lactose 
m6.4 <- quap(
  alist(
    K ~ dnorm( mu , sigma ) ,
    mu <- a + bL*L ,
    a ~ dnorm( 0 , 0.2 ) , 
    bL ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ) , data=d )

m6.5 <- quap(
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bF*F + bL*L , 
    a ~ dnorm( 0 , 0.2 ) , 
    bF ~ dnorm( 0 , 0.5 ) , 
    bL ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ),data=d ) 

precis( m6.3 ) 
precis( m6.4 )
precis( m6.5 )
```



Percent fat (perc.fat) and percent lactose (perc.lactose) are mirrors of each other (m6.3 and 6.4). Putting both in a model (6.5) gives posterior means of both closer to 0 than either individually - mutual information.

```{r}
pairs( ~ kcal.per.g + perc.fat + perc.lactose , data=d , col=rangi2 )
```

Very obvious from pairs plot, either helps, but neither does if you already know the other. Causal approach is the best approach.

```{r, fig.height=2}
dag0 <- dagitty( "dag {
  L <- D -> F
  L -> K
  F -> K
}")
coordinates(dag0) <- list( x=c(L=0,D=1,K=1,F=2) , y=c(L=0,D=0,K=1,F=0) ) 
drawdag( dag0 )
```


Central tradeoff is on $D$, the density of milk (nonobserved) - fat and lactose are from that (mediators). 

_Non-Identifiability_ - structure of data and model do not make it possible to estimate a parameter's value. Can be coding, more often though just nature making it difficult.

## 6.2 Post-treatment Bias

_Omitted Variable Bias_ - Mistaken inferences that arise from omitting predictor variables

_Post-treatment Bias_ - Mistaken inference that arise from including variables that are the consequences of others. 

Name "post-treatment" stems from experimental design, adding variables to model that are a result of the experiment, not truly independent. E.g. plant growing experiment, outcome of interest is final height but some plants grow fungus during the experiment - should not include this.

```{r}
set.seed(71)
# number of plants 
N <- 100
# simulate initial heights 
h0 <- rnorm(N,10,2)
# assign treatments and simulate fungus and growth 
treatment <- rep( 0:1 , each=N/2 )
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 ) 
h1 <- h0 + rnorm(N, 5 - 3*fungus)
# compose a clean data frame
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus ) 
#precis(d)
```

### A prior is born

Best approach is to pretend you dont' have the data generating process.

For this example, height at $t=1$ is height than that at $t=0$, so put parameters on a scale of _proportion_ of inital height - allows easier prior setting.

\begin{align*}
  h_{1,i} &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= h_{0,i} \times p \\
\end{align*}

So $p=2$ means at $t=1$ it doubles in height, $p=1$ means same height. Set this prior to 1, to allow for negative growth, force $p>0$ since a proportion - use Log-Normal since it's always positive.

```{r}
sim_p <- rlnorm( 1e4 , 0 , 0.25 ) 
#precis( data.frame(sim_p) )
```

Covers everything from 40% shrinking to 50% growth, centered at 1.

```{r}
m6.6 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ), 
    mu <- h0*p,
    p ~ dlnorm( 0 , 0.25 ), 
    sigma ~ dexp( 1 )
), data=d ) 
precis(m6.6)
```
On average, about 40% growth.

Now the model we said would be bad before - adding fungus growth

\begin{align*}
  h_{1,i} &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= h_{0,i} \times p \\
  p &= \alpha + \beta_T T_i + \beta_F F_i \\
  \alpha &\sim \text{Log-Normal}(0,0.25)\\
  \beta_T &\sim \text{Normal}(0,0.5)\\
  \beta_F &\sim \text{Normal}(0,0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

Sample model, prirors on slopes may be too flat (between -1 and +1), but otherwise proportion of growth defined.

```{r}
m6.7 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment + bf*fungus, 
    a ~ dlnorm( 0 , 0.2 ) ,
    bt ~ dnorm( 0 , 0.5 ),
    bf ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.7)
```

Nearly same posterior. $\beta_T$  is 0, tight interval - not associated. $\beta_F$ is negative - hurts growth. What happened?

### Blocked by consequence

Fungus is a consequence of treatment - "post-treatment variable." Controlling for fungus, model answers the question: "Once we already know if a plant developed fungus, does soil treatment matter?" Which, no, it's 0. But we care about treatment.

```{r}
m6.8 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment,
    a ~ dlnorm( 0 , 0.2 ), 
    bt ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.8)
```

Now the model is a lot more sensible 

- Makes sense to control for pre-treatment effects (e.g. initial height) as they might mask causal influence
- Post-treatment inclusion can mask the treatment itself

### Fungus and _d_-separation

Draw the DAG:

```{r, fig.height=2}
plant_dag <- dagitty( "dag {
  H_0 -> H_1 
  F -> H_1 
  T -> F
}")
coordinates( plant_dag ) <- list( x=c(H_0=0,T=2,F=1.5,H_1=1) ,
                                  y=c(H_0=0,T=0,F=0,H_1=0) )
drawdag( plant_dag )
```

$T$ influences $F$ which influences $H_1$. By including $F$, you "block" $T$, or another way to say this is that conditioning onf $F$ induces _d-separation_ (d for directional). This means that some variables on a directed graph are independent of others, no path connecting them. Going back to _conditional independencies:_

```{r}
impliedConditionalIndependencies(plant_dag)
```

The last one is important - our outcome is independent of $T$, given conditioning on $F$.

```{r, fig.height=2}
moisture_dag <- dagitty( "dag {
  H_0 -> H_1 
  H_1 <- M -> F
  F -> T
}")
coordinates( moisture_dag ) <- list( x=c(H_0=0,T=2,F=1.5,H_1=0.5,M=1) ,
                                     y=c(H_0=0,T=0,F=0,H_1=0,M=0.5) )
drawdag( moisture_dag )
```

Another DAG - Now, an onobserved moisture variable affects both height and fungus growth. In this case (with hypothetical plant unaffected by a fungus), $H1$ and $T$ will appear to be not associated, but then the conditioning on $F$ suddenly fools you into thinking there's an association.

```{r}
set.seed(71)
N <- 1000
h0 <- rnorm(N,10,2)
treatment <- rep( 0:1 , each=N/2 )
M <- rbern(N)
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 + 0.4*M )
h1 <- h0 + rnorm( N , 5 + 3*M )
d2 <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )

m6.7_2 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment + bf*fungus, 
    a ~ dlnorm( 0 , 0.2 ) ,
    bt ~ dnorm( 0 , 0.5 ),
    bf ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp( 1 )
  ), data=d2 ) 
precis(m6.7_2)

m6.8_2 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment,
    a ~ dlnorm( 0 , 0.2 ), 
    bt ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
  ), data=d2 ) 
precis(m6.8_2)
```

Next section fleshes this effect out more.

## 6.3 Collider Bias

The initial claim in the chapter of negative correlation between newsworthinessa and trustworthiness resulting from selection process is a type of _collider bias_.

```{r, fig.height=2}
collider_dag <- dagitty( "dag {
  T -> S <- N
}")
coordinates( collider_dag ) <- list( x=c(T=0,S=1,N=2) ,
                                     y=c(T=0,S=0,N=0) )
drawdag( collider_dag )
```

A "collider" is when 2 arrows enter, as in $S$ for that DAG. Conditioning on colliders have statistical, not causal, associations betweenamong causes - learning a proposal was selected for trustworthiness provides information about newsworthiness if you know a proposal was selected (if low $T$, high $N$).

### Collider of false sorrow

Turn to problem of happiness and age. Say happiness, $H$, is a fixed parameter at birth, but influences events - happier is more likely to get married $M$. Age, $A$, is also correlated to marriage (older = more likely), so get same collider dag:

```{r, fig.height=2}
collider_dag <- dagitty( "dag {
  H -> M <- A
}")
coordinates( collider_dag ) <- list( x=c(T=0,S=1,N=2) ,
                                     y=c(T=0,S=0,N=0) )
drawdag( collider_dag )
```

Even though no causal assocaition between happiness and age, if conditioned on marriage, induces an association.

```{r}
library(rethinking)
d <- sim_happiness( seed=1977 , N_years=1000 ) 
#precis(d)
```

Simulation of that data: 1300 people from 0-65. Want to ask if age is related to happiness - try multiple regression with linear model:

\begin{align*}
  \mu_i = \alpha_{\text{MID}[i]} + \beta_A A_i
\end{align*}

MID here is marriage status, 1 is single and 2 is marriaed. Only want older than 18 years old, so correct it 0 to 1, 0 is 18 and 1 is 65. Happiness is arbitrary between -2 and 2.

```{r}
d2 <- d[ d$age>17 , ] # only adults 
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )

d2$mid <- d2$married + 1 
m6.9 <- quap(
  alist(
    happiness ~ dnorm( mu , sigma ), 
    mu <- a[mid] + bA*A,
    a[mid] ~ dnorm( 0 , 1 ),
    bA ~ dnorm( 0 , 2 ),
    sigma ~ dexp(1)
  ) , data=d2 ) 
precis(m6.9,depth=2)
```

This model says age is negatively associated with happiness - try ommitting marriage

```{r}
m6.10 <- quap( 
  alist(
    happiness ~ dnorm( mu , sigma ), 
    mu <- a + bA*A,
    a ~ dnorm( 0 , 1 ),
    bA ~ dnorm( 0 , 2 ),
    sigma ~ dexp(1) ) , 
  data=d2 )
precis(m6.10)
```

No association if marriage not included. Highlights selection bias - more people get married as time goes on, so mean among married people approaches population average of zero; inverse for unmarried, happier people migrate over to married population.


### The haunted DAG

Not always easy to see a collider because of unmeasured causes, which can still induce it. Example - infer influence of parents $P$ and grandparents $G$ on education of children $C$. Think a simple dag where $G$ influences $P$ which both influence $C$. But possibly there's an unobservable $U$ confounder that's not shared by grandparents - say a neighborhood effect after parents moved

```{r, fig.height=2}
parent_dag <- dagitty( "dag {
  C <- G -> P
  P -> C
  P <- U -> C
}")
coordinates( parent_dag ) <- list( x=c(G=0,P=1,C=1,U=2) ,
                                     y=c(G=0,P=0,C=1,U=0.5) )
drawdag( parent_dag )
```
  
  
  If conditiditioned on $P$, bias inference on $G \rightarrow$ C$, even without measureing $U$. Example:
  
```{r}
N <- 200 # number of grandparent-parent-child triads
b_GP <- 1 # direct effect of G on P
b_GC <- 0 # direct effect of G on C
b_PC <- 1 # direct effect of P on C
b_U<-2 #direct effect of U on P and C
  
set.seed(1)
U <- 2*rbern( N , 0.5 ) - 1
G <- rnorm( N )
P <- rnorm( N , b_GP*G + b_U*U )
C <- rnorm( N , b_PC*P + b_GC*G + b_U*U ) 
d <- data.frame( C=C , P=P , G=G , U=U )
```

Set $G$ effect as 0 for effect, $U$ is binary. Try to measure grandparent influence now

```{r}
m6.11 <- quap( 
  alist(
    C ~ dnorm( mu , sigma ), 
    mu <- a + b_PC*P + b_GC*G,
    a ~ dnorm( 0 , 1 ), 
    c(b_PC,b_GC) ~ dnorm( 0 , 1 ), 
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.11)
```
    

Grandparents have huge negative effect, parents have twice as large as it should be. Regression not wrong but causal interpretation is. If you make a plot of $C$ vs $G$, there's 2 sets, one for good neighborhoods $U=1$ and one for bad $U=-1$ - but selecting those of similar those with similar education causes a negative trend. Knowing $P$, learning $G$ invisibly tells us about the neighborhood - how to solve? Measure $U$.

```{r}
m6.12 <- quap( 
  alist(
    C ~ dnorm( mu , sigma ),
    mu <- a + b_PC*P + b_GC*G + b_U*U, 
    a ~ dnorm( 0 , 1 ), 
    c(b_PC,b_GC,b_U) ~ dnorm( 0 , 1 ), 
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.12)
```

Which matches the simulated data. This effet is _Simpson's Paradox_ - adding a predictor can reverse direction of association between another predictor and outcome.

## 6.4  Confronting Confounding



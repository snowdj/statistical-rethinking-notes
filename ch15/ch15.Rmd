# Chapter 15 - Missing Data and Other Opportunities
```{r, include=FALSE}
library(rethinking)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

An advantage of Bayesian inference is that it gets rid of the need to be clever. Burnt pancake example of conditional probability - think of ways you could be looking at the data (count _sides_ of pancakes), rather than outcome-based (which pancake?). We can also think of BDA as:

$$
  \Pr (\text{want to know}|\text{already know})
$$

Probability theory is hard to interpret and apply, often requiring cleverness, but Bayesian approach applies conditional probability - once we define assumptions, probability just does the rest.

This chapter shows two applications of assume-and-deduce strategy - incorporating _measurement error_ and then estimation of missing data through _Bayesian Imputation_. 

## 15.1 - Measurement Error 

Using divorce data. Both divorce variable and marriage rate have standard errors provided

```{r}
data("WaffleDivorce")
d<-WaffleDivorce
head(d)
```

Shown in the .SE columns. Plotting this,

```{r}
# points
plot( d$Divorce ~ d$MedianAgeMarriage , ylim=c(4,15) ,
    xlab="Median age marriage" , ylab="Divorce rate" )

# standard errors
for ( i in 1:nrow(d) ) {
    ci <- d$Divorce[i] + c(-1,1)*d$Divorce.SE[i]
    x <- d$MedianAgeMarriage[i]
    lines( c(x,x) , ci )
}
```


Error varies due to state size. Some states are more confident, so we should weight those higher.

### Error on the Outcome

Think of how we'd generate data, what would we want errors to look like? Decrease as log population increases.

A DAG:

```{r}
library(dagitty)
dag <- dagitty("dag{ 
  A-> M -> D
  A-> D -> D_obs <- e_D
  D [unobserved]
  e_D [unobserved]
  }")
coordinates(dag) <- list( x=c(A=0,M=1,D=1,D_obs=2,e_D=3) , y=c(A=1,M=0,D=2,D_obs=2,e_D=2) )
drawdag(dag)
```


True divorce rate cannot be observed, but we do have a observed divorce rate that's a function of the true divorce rate and the error on it. Most regressions in general are really looking at $D_{obs}$ as a proxy for $D$.

How to put into a model - like a simulation but in reverse, assign a distribution to observations and draw from it. For examplue, if a measurement is 10 meters with standard Guassian deviation of 2, implies 

$$
  y \sim \text{Normal}(10,2)
$$

The key insight - if we don't know the true value, we can put a parameter and let Bayes do the rest.

Going back to divorce rate, we can use the parameter:

$$
  D_{obs,i} \sim \text{Normal}(D_{true,i},D_{SE,i})
$$

Full model:

\begin{align*}
  D_{obs,i} &\sim \text{Normal}(D_{true,i},D_{SE,i})\\
  D_{true,i} &\sim \text{Normal}(\mu_i, \sigma)\\
  \mu_i &= \alpha + \beta_A A_i + \beta_M M_i \\
  \alpha &\sim \text{Normal}(0,0.2)\\
  \beta_A &\sim \text{Normal}(0,0.5)\\
  \beta_M &\sim \text{Normal}(0, 0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

Effectively just a linear regression, but you have a connecting term from observation to true parameter value.

```{r, results=FALSE, message=FALSE, warning=FALSE}
dlist <- list(
    D_obs = standardize( d$Divorce ),
    D_sd = d$Divorce.SE / sd( d$Divorce ),
    M = standardize( d$Marriage ),
    A = standardize( d$MedianAgeMarriage ),
    N = nrow(d)
)

m15.1 <- ulam(
    alist(
        D_obs ~ dnorm( D_true , D_sd ),
        vector[N]:D_true ~ dnorm( mu , sigma ),
        mu <- a + bA*A + bM*M,
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=dlist , chains=4 , cores=4 , cmdstan=TRUE)
```

```{r}
precis(m15.1)
```

If we consider the posterior means, previously ```bA``` was -1, now it's about half that, but still reliably negative, the error reduced the association - not always the case.

If you plot the previous vs new model, one thing to notice is that the less certain estimates are more susceptible to shrinkage than the more certain ones.

### Error on Both the Outcome and Predictor

If there's error on predictors, the approach is the same.

```{r}
library(dagitty)
dag <- dagitty("dag{ 
  A-> M -> D
  A-> D -> D_obs <- e_D
  M -> M_obs <- e_M
  M [unobserved]
  e_M [unobserved]
  D [unobserved]
  e_D [unobserved]
  }")
coordinates(dag) <- list( x=c(A=0,M=1,D=1,D_obs=2,e_D=3,M_obs=2,e_M=3) , y=c(A=1,M=0,D=2,D_obs=2,e_D=2,M_obs=0,e_M=0) )
drawdag(dag)
```

With model:

\begin{align*}
  D_{obs,i} &\sim \text{Normal}(D_{true,i},D_{SE,i})\\
  D_{true,i} &\sim \text{Normal}(\mu_i, \sigma)\\
  \mu_i &= \alpha + \beta_A A_i + \beta_M M_{true,i} \\
  M_{obs,i} &\sim \text{Normal}(M_{true,i}, M_{SE,i})\\
  M_{true,i} &\sim \text{Normal}(0,1)\\
  \alpha &\sim \text{Normal}(0,0.2)\\
  \beta_A &\sim \text{Normal}(0,0.5)\\
  \beta_M &\sim \text{Normal}(0, 0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}


```{r, results=FALSE, message=FALSE, warning=FALSE}
dlist <- list(
    D_obs = standardize( d$Divorce ),
    D_sd = d$Divorce.SE / sd( d$Divorce ),
    M_obs = standardize( d$Marriage ),
    M_sd = d$Marriage.SE / sd( d$Marriage ),
    A = standardize( d$MedianAgeMarriage ),
    N = nrow(d)
)

m15.2 <- ulam(
    alist(
        D_obs ~ dnorm( D_true , D_sd ),
        vector[N]:D_true ~ dnorm( mu , sigma ),
        mu <- a + bA*A + bM*M_true[i],
        M_obs ~ dnorm( M_true , M_sd ),
        vector[N]:M_true ~ dnorm( 0 , 1 ),
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp( 1 )
    ) , data=dlist , chains=4 , cores=4 , cmdstan = TRUE)
```

```{r}
precis(m15.2)
```

Output is pretty similar, but we did get updated estimates of marriage rate.

```{r}
post <- extract.samples( m15.2 )
D_true <- apply( post$D_true , 2 , mean )
M_true <- apply( post$M_true , 2 , mean )
plot( dlist$M_obs , dlist$D_obs , pch=16 , col=rangi2 ,
    xlab="marriage rate (std)" , ylab="divorce rate (std)" )
points( M_true , D_true )
for ( i in 1:nrow(d) )
    lines( c( dlist$M_obs[i] , M_true[i] ) , c( dlist$D_obs[i] , D_true[i] ) )
```


The big takeaway - if you have a big distribution of values, don't use a single value for regression, use the distribution.

### Measurement Terrors

Errors can be difficult to manage, especialy if correlated. Imagine for some variable $P$,

```{r}
dag <- dagitty("dag{ 
  A-> M -> D
  A-> D -> D_obs <- e_D
  M -> M_obs <- e_M
  e_M <- P -> e_D
  M [unobserved]
  e_M [unobserved]
  D [unobserved]
  e_D [unobserved]
  }")
coordinates(dag) <- list( x=c(A=0,M=1,D=1,D_obs=2,e_D=3,M_obs=2,e_M=3,P=4) , y=c(A=1,M=0,D=2,D_obs=2,e_D=2,M_obs=0,e_M=0, P=1) )
drawdag(dag)
```


If we regress $D_{obs}$ on $M_{obs}$, there's an open, non-causal path. A possible solution is to model true $D$ and $M$, if we know the measurement process.

There's also the case where one variable influences the error and creates a non-causal path:
```{r}
dag <- dagitty("dag{ 
  A-> M -> D
  A-> D -> D_obs <- e_D
  e_D <- M -> M_obs <- e_M
  M [unobserved]
  e_M [unobserved]
  D [unobserved]
  e_D [unobserved]
  }")
coordinates(dag) <- list( x=c(A=0,M=1,D=1,D_obs=2,e_D=3,M_obs=2,e_M=3) , y=c(A=1,M=0,D=2,D_obs=2,e_D=2,M_obs=0,e_M=0) )
drawdag(dag)
```

This might happen if marriages are rare, then fewer divorce possibilities, then smaller sample to measure divorce, so ultimately small $M$ induces large $e_D$.

Anther problem is when a causal variable is measured less precisely than a non-causal. Say we know $D$ and $M$ well but $A$ has a lot of error, and no causality between $M$ and $D$:

```{r}
dag <- dagitty("dag{ 
  e_A [unobserved]
  A [unobserved]
  e_A -> A_obs <- A
  M <- A -> D
  }")
coordinates(dag) <- list( x=c(e_A=0, A_obs=1, A=2, D=3, M=3) , y=c(e_A=1, A_obs=1, A=1, D=2, M=0) )
drawdag(dag)
```

Here, a naive regression of $D$ on $A_{obs}$ and $M$ will suggest $M$ influences $D$ - $M$ basically functions as proxy $A$.

## 15.2 - Missing Data

What to do if data is missing?

Common behavior is to just drop all cases - _Complete Case Analysis_. It wastes data though.

Instead _impute_ missing data. Generative models tell you weather the process that produced missing values will also prevent the identification of causal effects. You can add missing contributions to a DAG too.

Rethinking - Missing data are meaningful: missing values depend on context. If someone omits income on a form, often this may mean a particularly low or high value. In ecology, if you haven't observed a species it might mean it's not there, it also might mean you just haven't seen one. _Occupancy models_ try to take this into account.

### DAG ate my homework

Sample of students who own dogs - produce homework $H_p$, influenced by how much they study $S$. Dogs eat some of the homework ($D$), so we can't see the true distribution, only those returned $H_m$. Can think of several causal scenarios:

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
  S -> H_p -> H_m <-D
  H_p [unobserved]
  }")
coordinates(dag) <- list( x=c(S=0, D=0, H_p=1, H_m=1) , y=c(S=0, D=1, H_p=0, H_m=1) )
drawdag(dag)
```

Simplest shown - completely random.

```{r}
# Simulate 100 students with attributes and homework
N <- 100
S <- rnorm( N )
H <- rbinom( N , size=10 , inv_logit(S) )

# Let dogs randomly eat homework sets
D <- rbern( N ) # dogs completely random
Hm <- H # All homework
Hm[D==1] <- NA # Dropout those eaten as NANs
```

Are these missing values specifically problematic? Depends - is outcome $H$ independent of $D$? If so, then the missing values don't change the overall distribution in a systematic way. Estimation is less efficient, but doesn't induce a bias.

But what if $S$ influences $D$? More studying causes dogs to want to eat homework.

```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
  S -> H_p -> H_m <-D
  S-> D
  H_p [unobserved]
  }")
coordinates(dag) <- list( x=c(S=0, D=0, H_p=1, H_m=1) , y=c(S=0, D=1, H_p=0, H_m=1) )
drawdag(dag)
```

This scenario is far more problematic, students that study more than average have their homeworks eaten more. Backdoor non-causal path from $H_p \rightarrow H_m \leftarrow D \leftarrow S$. If left unclosed, confound inference along $S\rightarrow H_p$, so just need to condition on $S$ (we're doing that anyway). Still need to keep in mind the danger if we get functions or distributions wrong.




## R code 15.10
D <- ifelse( S > 0 , 1 , 0 )
Hm <- H
Hm[D==1] <- NA

## R code 15.11
set.seed(501)
N <- 1000
X <- rnorm(N)
S <- rnorm(N)
H <- rbinom( N , size=10 , inv_logit( 2 + S - 2*X ) )
D <- ifelse( X > 1 , 1 , 0 )
Hm <- H
Hm[D==1] <- NA



```{r, fig.align="center", fig.height=2}
dag <- dagitty("dag{ 
  S -> H_p -> H_r <-D
  H_p [unobserved]
  }")
coordinates(dag) <- list( x=c(S=0, D=0, H_p=1, H_r=1) , y=c(S=0, D=1, H_p=0, H_r=1) )
drawdag(dag)
```

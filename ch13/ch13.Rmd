# Chapter 13 - Models With Memory
```{r, include=FALSE}
library(rethinking)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

Models forget often - as moving from one cluster (individual, group, etc) to another, they forget about previous data. We see this in any of the dummy variable models from before.

Robot-cafe problem: wants to estimate waiting time at each of two cafes, starts with prior of 5 minutes, deviation of 1. After the first, observes a wait time of 4 minutes, then moves onto the second - what should the prior be? Represent the population of cafes and learn about that - a parameter for each cafe and parameters to describe the population.

This leads to multilevel models, which learn about populations and thus have "memory." Depending on variation, also pools information across clusters. This leads to several benefits.

1. Improved estimates for repeat sampling
2. Improved estimates for imbalance in sampling
3. Estimates of variation
4. Avoid averaging, retain variation

**When it comes to regression, multilevel regression deserves to be the default approach**.

Some costs - you have to make new assumptions about distributions, but maximum entropy helps with this. Estimation is hard, MCMC helps. Hard to understand because predictions are made at different levels.

Also known as _hierarchical_ or _mixed effects_ models. Parameters are commonly called _random effects_.

## 13.1 - Example Multilevel Tadpoles

Looking at [tadpole mortality](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/04-0535).

```{r}
library(rethinking)
data(reedfrogs)
d <- reedfrogs
str(d)
```

The target is survival (surv) out of an initial count, density. Each row is a "tank" or experimental environment - a _cluster_variable.

A multilievel model allows unique treatment for each tank while still retaining cross-information. We will use a _varying intercepts_ model, the simplest kind of _varying effects_.

First for comparison, using regularizing priors:

\begin{align*}
  S_i &\sim \text{Binomial}(N_i, p_i)\\
  \text{logit}(p_i) &= \alpha_{\text{TANK}[i]}\\
  \alpha_j &\sim \text{Normal}(0,1.5) \text{   , for  } j=1..48
\end{align*}

The second line gives unique log-odds for each tank

```{r, results=FALSE, message=FALSE, warning=FALSE}
# make the tank cluster variable
d$tank <- 1:nrow(d)

dat <- list(
    S = d$surv,
    N = d$density,
    tank = d$tank )

# approximate posterior
m13.1 <- ulam(
    alist(
        S ~ dbinom( N , p ) ,
        logit(p) <- a[tank] ,
        a[tank] ~ dnorm( 0 , 1.5 )
    ), data=dat , chains=4 , log_lik=TRUE, cmdstan = TRUE )
```


This gives 48 different intercepts, as we've done before.

Next, we'll do the multilevel model:

\begin{align*}
  S_i &\sim \text{Binomial}(N_i, p_i)\\
  \text{logit}(p_i) &= \alpha_{\text{TANK}[i]}\\
  \alpha_j &\sim \text{Normal}(\bar{\alpha}, \sigma)\\
  \bar{\alpha} &\sim \text{Normal}(0, 1.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

Line 3 here has changed, now the adaptive prior, with parameters $\bar{\alpha}$, an average, and $\sigma$. This means as a prior, our intercepts start by assuming the average and standard deviations of the population. Note this has two _levels_ - top level is outcome, $S$, next level are its parameters, including $\alpha$, then the following level are the priors of $\alpha$ - these are _hyperparameters_ (parameters of parameters), and priors are _hyperpriors_.


```{r,results=FALSE, message=FALSE, warning=FALSE}
m13.2 <- ulam(
    alist(
        S ~ dbinom( N , p ) ,
        logit(p) <- a[tank] ,
        a[tank] ~ dnorm( a_bar , sigma ) ,
        a_bar ~ dnorm( 0 , 1.5 ) ,
        sigma ~ dexp( 1 )
    ), data=dat , chains=4 , log_lik=TRUE ,cmdstan = TRUE)
```

This provides 50 parameters - one for each tank plus two hyperparameters.

```{r}
compare(m13.1, m13.2)
````

The multilevel model here has only ~21 effective parameters - prior assigned to each intercept shrinks them toward the mean $\bar{alpha}$. This is a _regularizing prior_, similar to other chapters, but the regularization _is learned from data_. The effective parameters are lower than the non-multilevel model, despite more actual parameters, because the adaptive regularization is more aggressive.

```{r}
# extract Stan samples
post <- extract.samples(m13.2)

# compute mean intercept for each tank
# also transform to probability with logistic
d$propsurv.est <- logistic( apply( post$a , 2 , mean ) )

# display raw proportions surviving in each tank
plot( d$propsurv , ylim=c(0,1) , pch=16 , xaxt="n" ,
    xlab="tank" , ylab="proportion survival" , col=rangi2 )
axis( 1 , at=c(1,16,32,48) , labels=c(1,16,32,48) )

# overlay posterior means
points( d$propsurv.est )

# mark posterior mean probability across tanks
abline( h=mean(inv_logit(post$a_bar)) , lty=2 )

# draw vertical dividers between tank densities
abline( v=16.5 , lwd=0.5 )
abline( v=32.5 , lwd=0.5 )
text( 8 , 0 , "small tanks" )
text( 16+8 , 0 , "medium tanks" )
text( 32+8 , 0 , "large tanks" )
```

Blue points are raw proportions from observed counts, black are the varying intercept medians. The 80% line is estimated median survival proportion in population of tanks, vertical lines are initial tadpole counts (10, 25, 35 left to right).

Multilevel estimates are closer to 80% median than raw empirical estimate - this is called _shrinkage_, resulting from regularization. More prevalent for smaller population tanks (left). Shrinkage is also stronger the further from the global average.

This is a result of _pooling_ - each tank provides information that can be used to improve estimation of other tanks.

Survival distribution:

```{r}
# show first 100 populations in the posterior
plot( NULL , xlim=c(-3,4) , ylim=c(0,0.35) ,
    xlab="log-odds survive" , ylab="Density" )
for ( i in 1:100 )
    curve( dnorm(x,post$a_bar[i],post$sigma[i]) , add=TRUE ,
    col=col.alpha("black",0.2) )

# sample 8000 imaginary tanks from the posterior distribution
sim_tanks <- rnorm( 8000 , post$a_bar , post$sigma )

# transform to probability and visualize
dens( inv_logit(sim_tanks) , lwd=2 , adj=0.1 )
```

## 13.2 - Varying Effects and the Underfitting/Overfitting Trade-Off

Varying intercepts are adaptively regularized estimates - a major benefit is that they provide more accurate estimates of the individual intercepts, and also the means. The reason is in the under/overfitting trade-off.

To understand consider:

1. Complete pooling - population of ponds is invariant; common intercept for all. This will use a lot of data for the $\alpha$ estimate, but unlikely to match any particular pond, as a result underfits.
2. No pooling - each model is unique and provides no information about others. Little data for each estimate, and overfit to the data. Errors are high and possibly infinite due to overfitting.
3. Partial pooling - adaptive regularizing prior. Less underfit than grand mean, less overfit than no-pooling

Important here though to simulate data to really understand the model - rest of this section.

### The model

\begin{align*}
  S_i &\sim \text{Binomial}(N_i, p_i)\\
  \text{logit}(p_i) &= \alpha_{\text{TANK}[i]}\\
  \alpha_j &\sim \text{Normal}(\bar{\alpha}, \sigma)\\
  \bar{\alpha} &\sim \text{Normal}(0, 1.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

Same as before (verbage changes from tanks to ponds in chapter).

Simulation needs 
- $\bar{\alpha}$, average log-odds of survival in entire population
- $\sigma$, standard deviation of the distribution of log-odds of survival
- $\alpha$, vector of individual pond intercepts
- $N_i$, sample size for each pond

### Assign values to parameters

```{r}
# Initalize values
a_bar <- 1.5
sigma <- 1.5
nponds <- 60
Ni <- as.integer( rep( c(5,10,25,35) , each=15 ) )

# Simulate sample and put in dataframe
set.seed(5005)
a_pond <- rnorm( nponds , mean=a_bar , sd=sigma )
dsim <- data.frame( pond=1:nponds , Ni=Ni , true_a=a_pond )

```


### Simulate survivors

Probability of survival is implied by logit definition:

$$
  p_i = \frac{ \exp(\alpha_i)}{1+\exp(\alpha_i)}
$$

```{r}
# Generate simulated survival count
dsim$Si <- rbinom( nponds , prob=logistic(dsim$true_a) , size=dsim$Ni )
```

### Compute non-pooling estimates

Pretty straightforward

```{r}
dsim$p_nopool <- dsim$Si / dsim$Ni
```


### Compute partial-pooling estimates


```{r,results=FALSE, message=FALSE, warning=FALSE}
dat <- list( Si=dsim$Si , Ni=dsim$Ni , pond=dsim$pond )
m13.3 <- ulam(
    alist(
        Si ~ dbinom( Ni , p ),
        logit(p) <- a_pond[pond],
        a_pond[pond] ~ dnorm( a_bar , sigma ),
        a_bar ~ dnorm( 0 , 1.5 ),
        sigma ~ dexp( 1 )
    ), data=dat , chains=4 , cmdstan = TRUE)
```

```
precis( m13.3 , depth=2 )
```
Will give a long table of intercepts. Add this to the dataframe

```{r}
# Partial pooling predictions
post <- extract.samples( m13.3 )
dsim$p_partpool <- apply( inv_logit(post$a_pond) , 2 , mean )

# True survival probabilities
dsim$p_true <- inv_logit( dsim$true_a )

# Errors
nopool_error <- abs( dsim$p_nopool - dsim$p_true )
partpool_error <- abs( dsim$p_partpool - dsim$p_true )
```

And finally plotting our simulation

```{r}
plot( 1:60 , nopool_error , xlab="pond" , ylab="absolute error" ,
    col=rangi2 , pch=16 )
points( 1:60 , partpool_error )
```

With error rates given by:

```{r}
nopool_avg <- aggregate(nopool_error,list(dsim$Ni),mean)
partpool_avg <- aggregate(partpool_error,list(dsim$Ni),mean)
```


both are more accurate for larger ponds, as you would expect. Varying effects really have the stronger advantage at the left side of the plot due to the partial pooling. It isn't always better, just on average in the long run.

This is a manifestation of the fact earlier that small ponds shrink more toward the mean.

## 13.3 - More than one type of cluster

Often can and should use multiple types of clusters in one model

Go back to Chimpanzee example - each pull belongs to both an ```actor``` (index 1-7) and an experimental ```block``` that occurred on the same day.

_Cross-classified_ multilevel model - actors are not nested within unique blocks. If all pulls are on the same day, it would be instead _hierarchical_.

### Multilevel Chimpanzees

Adding varying intercepts to previous model - replace fixed regularizing prior to adaptive. Also add a second cluster type for block:

\begin{align*}
  L_i &\sim \text{Binomial}(1, p_i)\\
  \text{Logit}(p_i) &=  \alpha_{\text{ACTOR}[i]} + \gamma_{\text{BLOCK}[i]} + \Beta_{\text{TREATMENT}[i]}\\
  \beta_j &\sim \text{Normal}(0,0.5) \text{   , for  } j =1..4\\
  \alpha_j &\sim \text{Normal}(\bar{\alpha},\sigma_\alpha) \text{   , for  } j =1..7\\
  \gamma_j &\sim \text{Normal}(0,\sigma_\gamma) \text{   , for  } j =1..6\\
  \bar{\alpha} &\sim \text{Normal}(0,1.5)\\
  \sigma_\alpha &\sim \text{Exponential}(1)\\
  \sigma_\gamma &\sim \text{Exponential}(1)\\
\end{align*}

Each cluster has its own vector of parameters

- Actors, $\alpha$, length 7
- Blocks, $\gamma$, length 6

Each needs own standard deviation that adapts the amount of pooling across units, $\sigma_\alpha$ and $\sigma_\gamma$.

Unable to identify a separate mean for each varying intercept type since both are added to the same linear prediction, so there is only one global mean $\bar{\alpha}$.

Putting the model into code:

```{r,results=FALSE, message=FALSE, warning=FALSE}
library(rethinking)
data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition

dat_list <- list(
    pulled_left = d$pulled_left,
    actor = d$actor,
    block_id = d$block,
    treatment = as.integer(d$treatment) )

set.seed(13)
m13.4 <- ulam(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a[actor] + g[block_id] + b[treatment] ,
        b[treatment] ~ dnorm( 0 , 0.5 ),
      ## adaptive priors
        a[actor] ~ dnorm( a_bar , sigma_a ),
        g[block_id] ~ dnorm( 0 , sigma_g ),
      ## hyper-priors
        a_bar ~ dnorm( 0 , 1.5 ),
        sigma_a ~ dexp(1),
        sigma_g ~ dexp(1)
    ) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE, cmdstan = TRUE)
```

Looking at the raw stan code:

```{r}
stancode(m13.4)
```

And the outputs:

```{r}
precis( m13.4 , depth=1 )
plot( precis(m13.4,depth=1) ) # also plot
```

For whatever reason, depth 2 gives me a lot more parameters than the book; I've suppressed them. Some things I should note on that:

- ```n_eff``` varies a lot across parameters, common in complex models. Also a result of inefficient  sampling, covered in next section
- Comparing ```sigma_a``` to ```sigma_g``` gives a lot larger variation among actors (a) than across blocks (g). This tells us that adding blocks doesn't risk overfitting too much

Making a model specifically that ignores block and comparing:

```{r}
set.seed(14)
m13.5 <- ulam(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a[actor] + b[treatment] ,
        b[treatment] ~ dnorm( 0 , 0.5 ),
        a[actor] ~ dnorm( a_bar , sigma_a ),
        a_bar ~ dnorm( 0 , 1.5 ),
        sigma_a ~ dexp(1)
    ) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

compare( m13.4 , m13.5 )
```

Model 13.4 has 7 more true parameters but only 2 more effective parameters since the 6 block (g) parameters shrunk toward 0.

"Selecting" model here isn't the aim, the information we get is by comparing the two - including block basically doesn't matter, which we can see from ```sigma_g``` and ```g```. Model _comparison_ is the interesting thing, experimental design tells us the relevant causal model to inspect.

### Even more clusters

Might consider treatment effects look like ```a``` and ```g``` parameters - could we use partial pooling? Sure (Apparently though there's some push-back, traditional teaching is that varying effects are used for not-experimentally controlled variables. Treatment is "fixed" so use "fixed" effects). Use varying effects because they provide better inferences _regardless of how clusters arise_.  If units are _exchangeable_, meaning index values could be reassigned without changing meaning, then partial pooling can help.

In our case, 4 treatments with large data on each, so partial pooling doesn't help much.

## 13.4 - Divergent Transitions and Non-Centered Priors
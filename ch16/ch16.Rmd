# Chapter 16 - Generalized Linear Madness
```{r, include=FALSE}
library(rethinking)
library(dagitty)
library(bayesplot)
library(posterior)
library(ggplot2)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

Applied statistics has to apply to all sciences, and is more vague about models, focusing on average performance. They're not credible "scientific" models, more akin to geocentric descriptions. 

There are problems with the GLM plus DAG approach, not everything can be modeled as a GLM (linear combination of variables mapped to non-linear outcome). Sometimes parameters are fixed by theory. GLMs failure is hard to notice and learn.

This chapter - using scientific context to provide a causal model, _bespoke_ statistical models. Off the shelf models interrupt expertise, tools forbid use of expert knowledge.

## 16.1 - Geometric People

Height/weight example - weight does not _cause_ height, if anything opposite. We'll try to do better by approximating the person as a cylinder.

### Scientific Model

Volume of a cylinder is

$$
  V= \pi r^2 h
$$

Don't know radius but assume it's a constant proportion of height, $r=ph$, so

$$
  V = \pi (ph)^2 h = \pi p^2 h^3
$$

Then say weight is a proportion of volume 

$$
  W = kV = k \pi p^2 h^3.
$$

### Statistical Model

\begin{align*}
  W &\sim \text{Log-Normal}(\mu_i, \sigma)\\
  \exp(\mu_i) &= k \pi p^2 h_i^3\\
  k &\sim \text{ some prior}\\
  p &\sim \text{ some prior}\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

The first line is the distribution of the outcome, weight. It is positive and continuous, so chose Log-Normal, parameterized by mean, $\mu$. The median is $\exp(\mu_i)$, so this is what we assign. 

One big thing to note is that $k$ and $p$ are multiplied and we have no way to estimate anything other than their product - they're _not identifiable_. So we replace $kp^2$ with a new parameter $\theta$,

$$
  \exp (\mu_i) = \pi \theta h_i^3.
$$
An advantage here is that parameters have meanings, which we can use for priors. This is a bit harder now that we've put in $\theta$ though.

For $p$, it's the ratio of radius to height, $p=r/h$, so greater than 0. Definitely less than 1, people are usually less wide than tall, and probably less than 1/2, so we should put most of our density below that,

$$ p \sim \text{Beta}(2,18) $$
This way it's got mean $2/(2+18) = 0.1$.

Next $k$, proportion of volume to weight - really just translates measurement scales. If height is in cm and weight in kg, volume has units cm$^3$, so $k$ is $kg/cm^3$, or how many kilograms are in a cubic centimeter - something we could look up or measure on our own bodies.

If that's not a possibility though, can just get rid of measurement scales - divide height and weight by mean value:

```{r}
data(Howell1)
d <- Howell1

# Scale variables
d$w <- d$weight / mean(d$weight)
d$h <- d$height / mean(d$height)
```

These now have means of 1, so we let $w_i = h_i =1$, turning the formula to:

$$
  1 = k \pi p^2 1^3
$$

If we want $p < 0.5$, then $k>1$, so we constrain $k$ to be positive with a prior mean around 2,

$$
  k \sim \text{Exponential}(0.5)
$$

Codifying all of this:

```{r,  results=FALSE, message=FALSE, warning=FALSE}
m16.1 <- ulam(
  alist(
    w ~ dlnorm( mu , sigma),
    exp(mu) <- 3.141593 * k * p^2 * h^3,
    p ~ beta(2,18),
    k ~ exponential(0.5),
    sigma ~ exponential(1)
  ), data=d, chains=2, cores=2, cmdstan = TRUE
)
```


```{r}
plot(precis(m16.1))
```

```{r}
pairs(m16.1)
```


There's a narrow curved ridge in the posterior where $k$ and $p$ combinations produce the same product. Because we have informative priors, we can fit anyway

No reason that $k$ and $p$ aren't also functions of height or age, so $k$ isn't necessarily a constant (think changes in relative muscle mass), nor $p$. Helps understand predictions, plotting the predictions across height.

```{r}
h_seq <- seq(from=0, to=max(d$h),length.out=30)
w_sim <- sim(m16.1, data=list(h=h_seq))
mu_mean <- apply(w_sim, 2, mean)
w_CI <- apply(w_sim, 2, PI)
plot( d$h, d$w, 
      xlim=c(0,max(d$h)),ylim=c(0,max(d$w)), 
      col=rangi2, lwd=2,
      xlab="height (scaled)", ylab="weight (scaled)"
      )
lines(h_seq, mu_mean)
shade(w_CI, h_seq)
```

The model gets the general scaling relationship right, though it's not great at low values - $p$ and $k$ might be different for children.

The key-line takeaway: using the scientific approach, parameters have biological meanings, and these give us useful hints.

### GLLM in Disguise

Consider model related to logarithm of weight:

$$
  \log(w_i) = \mu_i = \log(k\pi p^2 h_i^3)
$$

Since multiplication becomes addition on log scale, rewrite:

$$
  \log(w_i) = \log(k) + \log(\pi) + 2\log(p) + 3\log(h_i)
$$

Linear regression on the log scale! Then $3\log(h_i)$ is a predictor with fixed coefficient of 3 from theory  that we didn't need to estimate. This just emphasizes the strength of GLMs, a lot of natural relationships _are_ GLMs.

## 16.2 Hidden Minds and Observed Behavior

The _inverse problem_ is common in scientific inference, figuring out causes from observations. This chapter will go from one with developmental psychology - how to determine if children are are influenced by the majority. 629 children saw 4 choose among 3 boxes. In each trial one was a majority choice (3), one a minority (1), and one unselected.

```{r}
library(rethinking)
data(Boxes)
precis(Boxes)
```

Outcome is y, taking value 1, 2, 3 - 1 is unchosen, 2 indicates majority, 3 is minority. Then `majority_first` shows if the majority color was demonstrated before the minority. Use outcome to infer strategies to choose a color.

```{r}
table(Boxes$y) / length(Boxes$y)
```

This does _not_ mean 45% are choosing the strategy of just following the majority - others exist. Pick at random, for example.

### Scientific Model

Think generatively - simulate cases where half pick at random, half follow majority
```{r}
set.seed(5)
N <- 30 # number of children

# half are random
# sample from 1,2,3 at random for each
y1 <- sample( 1:3 , size=N/2 , replace=TRUE )

# half follow majority
y2 <- rep( 2 , N/2 )

# combine and shuffle y1 and y2
y <- sample( c(y1,y2) )

# count the 2s
sum(y==2)/N
```

In this case about 2/3 pick the majority color, but half are actually doing that.

Consider 5 possible behaviors:

1. Follow Majority
2. Follow Minority
3. Maverick: pick the unselected color
4. Random
5. Follow first: pick the one that was demonstrated first

Why these? They seem plausible 

### Statistical Model

Remember, they run in reverse of generative models. Can't directly measure strategy, but each has a probability of producing each choice. So compute probability of each choice given parameters specified by the probability of each strategy, then get posterior back. Before we can do this, need to enumerate parameters, assign priors, and figure out technical issues.

Unobserved variables are probabilities for each of the strategies, we use a _simplex_. A vector of values that sum to a value (usually one), which we can give a Dirichlet prior. Use a weak uniform prior:

$$
  p \sim \text{Dirichlet}([4,4,4,4,4]).
$$

This means we don't expect any more or less probable than any other, if you make them larger the prior starts to say we expect them actually equal.

Now the likelihood. For each choice, $y_i$, each strategy $s$ implies a probability of seeing $y_i$, call this $\Pr(y_i|s)$, the probability of the data, conditional on assuming a strategy $s$. To get the probability for each possible $S$, average over the simplex:

$$
  \Pr(y_i) = \sum_{s=1}^{5} p_s \Pr(y_i|s)
$$

This is the weighted average of probabilities of $y_i$ conditional on each strategy $s$. This _marginalizes_ out the unknown strategy. In statistical fashion,

\begin{align*}
  y_i &\sim \text{Categorical}(\theta)\\
  \theta_j &= \sum_{s=1}^5 p_s \Pr(j|s) \text{   for  } j=1\dots 3 \\
  p &\sim \text{Dirichlet}([4,4,4,4,4])
\end{align*}

Where $\theta$ holds the probability of each behavior conditional on $p$.

### Coding the statistical model

Writing directly into stan, I'll write the model as a string, save as a `.stan` file, then use cmdstan to pre-compile. 

```{r}
model_code = "
data{
  int N;
  int y[N];
  int majority_first[N];
}
parameters{
  simplex[5] p;
}
model{
  vector[5] phi;
  
  // prior
  p ~ dirichlet( rep_vector(4,5) );
  
  // probability of data
  for ( i in 1:N ) {
      if ( y[i]==2 ) phi[1]=1; else phi[1]=0; // majority
      if ( y[i]==3 ) phi[2]=1; else phi[2]=0; // minority
      if ( y[i]==1 ) phi[3]=1; else phi[3]=0; // maverick
      phi[4]=1.0/3.0;                         // random
      if ( majority_first[i]==1 )             // follow first
          if ( y[i]==2 ) phi[5]=1; else phi[5]=0;
      else
          if ( y[i]==3 ) phi[5]=1; else phi[5]=0;
      
      // compute log( p_s * Pr(y_i|s )
      for ( j in 1:5 ) phi[j] = log(p[j]) + log(phi[j]);
      // compute average log-probability of y_i
      target += log_sum_exp( phi );
  }
}
"
m16_2_file <-"m16_2.stan"
writeLines(model_code, m16_2_file)
m16_2_cmdstan <- cmdstan_model(m16_2_file)
```

This needs 3 blocks of code. First the _data block_, which names observed variables and types. Next, the _parameters block_, like data, but for unobserved variables. Third, the _model bock_, which is the heart of our model. First, define phi to hold probability calculations, then define prior dirichlet. Then the tough part is the probability of data - loop over rows, for each row `i`, assign conditional on strategy. Also need to include the $p$ parameters, by adding each $\log(p_i)$, done in the last chunk above.

 
Now prepping data and running sampler.

```{r, results=FALSE, message=FALSE, warning=FALSE}
# prep data
dat_list <- list(
  N = nrow(Boxes),
  y = Boxes$y,
  majority_first = Boxes$majority_first
)

# Run stan using model defined above
m16.2 <- m16_2_cmdstan$sample(data=dat_list, chains=2, cores=2)
```

```{r}
# Get posterior draws
m16_2_draws <- m16.2$draws()
```

```{r}
posterior <- as_draws_df(m16_2_draws)
plot_title <- ggtitle("Posterior distributions","of various strategies")
mcmc_areas(posterior, 
           pars=c("p[1]","p[2]","p[3]","p[4]","p[5]"), prob=.89) + plot_title
```

This has been sort of made in an ad-hoc way due to having to use cmdstan to pre-compile the model. 45% chose the majority color, but with this construction, between 20-30% were only doing it because of copying (p[1]).

### State Space Models

Boxes model shown represents a broader class called _state space models_ - multiple hidden states that produce observations, changing over time. When discrete categories, it may be called a _Hidden Markov Model_ (HMM). Applies to many time series models, since true state is not observed, just noisy measures.

## 16.3 - Ordinary Differential Nut Cracking

Next example will look at Panda nuts, which are difficult for animals to open, but manageable with tools - model the development of nut opening skill among chimpanzees.

```{r}
data(Panda_nuts)
d <- Panda_nuts
head(d)
```

Each row is an bout of nut opening, interested in `nuts_opened`, the duration in `seconds` and `age`.

### Scientific model

Most basic model - only thing that matters is strength, which increases with age. In animals with terminal growth (reach stable adult body mass), size increases proportionally with distance remaining to maximum size,

$$
  \frac{dM}{dt} = k(M_{\text{max}}-M_t).
$$

Here, $k$ is a parameter measuring rate of skill gain with age. This differential equation is common in biology, with solution,

$$
  M_t = M_{\text{max}}(1-\exp(-kt)).
$$

What we care about though is strength, so we need to suppose strength is proportional to mass: $S_t = \beta M_t$, with $\beta$ as a constant of proportionality. 

Last, need to relate strength to rate of nut cracking. It helps in 3 ways - heavier usable tools, faster arm acceleration, more efficient lever arms. This implies increasing returns,

$$
  \lambda = \alpha S_t^\theta = \alpha(\beta M_{\text{max}}(1-\exp(-kt)))^\theta,
$$

with $\theta$ being an exponent larger than 1. The new $\alpha$ parameter expresses proportionality of strength to nut opening - newtons of force to nuts per second.

This is a giant soup of parameters, so let's simplify. First, rescale body mass scale, $M_{max}$, to equal 1. Next, $\alpha \beta^\theta$ is a giant scaling factor, so you can replace it with a single parameter $\phi$, yielding

$$
  \lambda = \phi (1-\exp(-kt))^\theta.
$$


### Statistical Model

Likelihood is straightforward, it's just Poisson distributed.

\begin{align*}
  n_i &\sim \text{Poisson}(\lambda_i)\\
  \lambda_i &= d_i \phi(1-\exp(-kt_i))^\theta
\end{align*}

Our outcome is $n_i$, number of nuts cracked, and the equation for $\lambda_i$ is the derived equation for rate of nuts cracked multiplied by duration/exposure $d_i$.

Priors require us to consider biology. Chimpanzees reach adult mass around 12 years of age, $k$ and $\theta$ need to accomplish this. $\phi$ needs to have a mean around the maximum rate... who really knows, but hazarding a guess, one per second, since multiple can be opened at once? Let's try:

\begin{align*}
  \phi &\sim \text{Log-Normal}(\log(1),0.1)\\
  k &\sim \text{Log-Normal}(\log(2),0.25)\\
  \theta &\sim \text{Log-Normal}(\log(5),0.25)
\end{align*}

All are set to log normal because they're both positive and continuous. Let's do a simulation test:

```{r}
N <- 1e4
phi <- rlnorm(N, log(1), 0.1)
k <- rlnorm(N, log(2), 0.25)
theta <- rlnorm(N, log(5), 0.25)

# Growth curve
plot(NULL, xlim=c(0,1.5), ylim=c(0,1), 
     xaxt="n", xlab="Age", ylab="Body Mass")
at <- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5)
axis(1, at=at, labels=round(at*max(Panda_nuts$age)))
for (i in 1:20) curve ( (1-exp(-k[i]*x)), add=TRUE, col=grau(),lwd=1.5)
```

```{r}
# Implied rate of nut opening curve
plot(NULL, xlim=c(0,1.5), ylim=c(0,1.2), 
     xaxt="n", xlab="Age", ylab="Nuts/Second")
at <- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5)
axis(1, at=at, labels=round(at*max(Panda_nuts$age)))
for (i in 1:20) curve ( phi[i]*(1-exp(-k[i]*x))^theta[i], add=TRUE, col=grau(),lwd=1.5)
```


Coding this model is possible with ulam,

```{r, results=FALSE, message=FALSE, warning=FALSE}
dat_list <- list(
  n = as.integer( Panda_nuts$nuts_opened),
  age = Panda_nuts$age / max(Panda_nuts$age),
  seconds = Panda_nuts$seconds
)

m16.4 <- ulam(
  alist(
    n ~ poisson(lambda),
    lambda <- seconds * phi * (1-exp(-k*age))^theta,
    phi ~ lognormal(log(1),0.1),
    k ~ lognormal(log(2),0.25),
    theta ~ lognormal(log(5),0.25)
  ), data=dat_list, chains=2, cores=2, cmdstan = TRUE
)
```


Interesting thing is the posterior developmental curve,

```{r}
post <- extract.samples(m16.4)
plot(NULL, xlim=c(0,1.5), ylim=c(0,1.5), 
     xaxt="n", xlab="Age", ylab="Nuts/Second")
at <- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5)
axis(1, at=at, labels=round(at*max(Panda_nuts$age)))

# Raw data
pts <- dat_list$n / dat_list$seconds
point_size <- normalize(dat_list$seconds)
points( jitter(dat_list$age), pts, col=rangi2, lwd=2, cex=point_size*3)

# Posterior curves
for (i in 1:30) with (post,
                      curve( phi[i]*(1-exp(-k[i]*x))^theta[i], add=TRUE, col=grau() ))
```

Blue points are raw data with size scaled by duration. 30 skill curves drawn from posterior distribution. They level off around the age of maximum body size, which is consistent with the idea that strength is the limiting factor.


### Covariates and individual differences

This model is pretty simple - you could extend it to covariates like sex. There are repeat observations of individuals, even across years, which could be used to estimate individual varying effects. You could also include a more realistic growth model of chimpanzees, which is well published.

Some parameters make sense varying by individual, others don't. $\theta$ is a feature of only physics, not individual, probably don't vary it - this is another reason to avoid GLMs.

## 16.4 - Population Dynamics
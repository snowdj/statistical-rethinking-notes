# Chapter 5 - The Many Variables & The Spurious Waffles
```{r, include=FALSE}
library(rethinking)
```

Title explanation: Waffle Houses are an index to disaster severity, opening quickly after severe events. Also correlated to divorce rates, this is a spurious correlation - it's a southern establishment, and the south has high divorce rates. Need to distinguish correlations from causation.

**Multiple Regression** - using multiple predictors to model an outcome. Useful for:

- Acting as a statistical control for confounds
- Multiple causation
- Interactions between variables (not dealt with in this chapter)

Also looking at **causal inference** this chapter, using graphs.

## 5.1 - Spurious Association

Test problem now divorce as a function of marriage rate, median marriage age. Straightforward linear regression.

```{r}
# load data and copy 
data(WaffleDivorce) 
d <- WaffleDivorce
# standardize variables
d$A <- scale( d$MedianAgeMarriage ) 
d$D <- scale( d$Divorce )

# Build model and sample
m5.1 <- quap( 
  alist(
    D ~ dnorm( mu , sigma ) ,
    mu <- a + bA * A ,
    a ~ dnorm( 0 , 0.2 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ) , data = d )

set.seed(10)
prior <- extract.prior( m5.1 )
mu <- link( m5.1 , post=prior , data=list( A=c(-2,2) ) )
plot( NULL , xlim=c(-2,2) , ylim=c(-2,2), 
      xlab="Median Marriage Age (std units)", ylab="Divorce Rate (std units)" )
for ( i in 1:50 ) lines( c(-2,2) , mu[i,] , col=col.alpha("black",0.4) )
```


Looking at posterior

```{r, warning=FALSE}
# compute percentile interval of mean
A_seq <- seq( from=-3 , to=3.2 , length.out=30 ) 
mu <- link( m5.1 , data=list(A=A_seq) )
mu.mean <- apply( mu , 2, mean)
mu.PI <- apply( mu , 2, PI)

# plot it all
plot( D ~ A , data=d , col=rangi2 , xlab="Median Marriage Age (std)", ylab="Divorce Rate (std")
lines( A_seq , mu.mean , lwd=2 )
shade( mu.PI , A_seq )
```

```{r,echo=FALSE}
d$M <- scale( d$Marriage ) 
m5.2 <- quap(
  alist(
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bM * M ,
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ) , data = d )
```


To make sense, we need to work on causation.

### Think before you regress

Three variables: Divorce rate ($D$), marriage rate ($M$), median age of marriage ($A$).

Set up a Directed Acyclic Graph (DAG)

```{r, fig.height=2}
library(dagitty)
dag5.1 <- dagitty( "dag {
  A -> D 
  A -> M 
  M -> D
}")
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) ) 
drawdag( dag5.1 )
```

Depicts the directions of influence - Age affects both marriages and divorce, marriages affect divorces. Have to account for each path, $A\rightarrow D$ and $A \rightarrow M \rightarrow D$.

$A \rightarrow M \rightarrow D$ path does little work, we know marriage is positively associated with divorce. The graph could also be something like this:

```{r, fig.height=2}
dag5.1 <- dagitty( "dag { D <- A -> M}")
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) ) 
drawdag( dag5.1 )
```

Also plausible - Carefully consider each DAG to know which is correct.

**Testable Implications** - 

Compare the two DAGs, the second gives the implication that once we've conditioned on $A$, $M$ tells us nothing more about $D$; $D$ is independent of $M$ conditional of $A$ $(D \perp \!\!\! \perp M|A)$

Code version of that:

```{r}
DMA_dag2 <- dagitty('dag{ D <- A -> M }') 
impliedConditionalIndependencies( DMA_dag2 )
```

Compared to:

```{r}
DMA_dag1 <- dagitty('dag{ D <- A -> M -> D }') 
impliedConditionalIndependencies( DMA_dag1 )
```

(no conditional indpendencies, so no output)

To test: need a model that conditions on $A$, so we can see weather that renders $D$ independent of $M$, multiple regression can do this.

Answers the question "Is there any additional value in knowing a variable, once I already know all of the other predictor variables?"

Note - often this question is framed as "controlling for one variable while estimating another," but statistical control is different from experimental so a bit sloppy.

#### Notation

Strategy:

1. Nominate predictor variables you want in linear model
2. For each, make a parameter that will measure it's association
3. Multiply

e.g.

\begin{align*}
  D_i &\sim \text{Normal}(\mu,\sigma)\\
  \mu_i &= \alpha + \beta_M M_i + \beta_A A_i\\
  \alpha &\sim \text{Normal}(0,0.2)\\
  \beta_M &\sim \text{Normal}(0,0.5)\\
  \beta_A &\sim \text{Normal}(0,0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

$M$ for marriage rate, $A$ for age of marriage. One sample way to read this is "a divorce rate is a function of its marriage rate or median age at marriage" (read $+$ as "or").

```{r}
d$M <- scale( d$Marriage )
m5.3 <- quap( 
  alist(
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bM*M + bA*A , 
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
    ) , data = d ) 
precis( m5.3 )
```

Visualization of posterior

```{r}
plot( coeftab(m5.1,m5.2,m5.3), par=c("bA","bM"))
```

89% compatibility shown, top is bA, bottom is bM. 

- bA doesn't move much, uncertainty grows
- bM only associated with divorce when age is missing from the model

So there is little to no additional predictive power in knowing the rate of marriage in a state/no direct causal path from marriage rate to divorce rate. Meaning, model 2 is the better one.


#### Plotting multivariate posteriors

**Predictor residual plots** - useful for understanding statistical model

Model has 2 predictors: $M$, marriage rate and $A$, median age. For residuals, you use the other predictor to model it. Residuals are found by subtracting observed from model.

\begin{align*}
  M_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= \alpha + \beta_A A_i\\
  \alpha &\sim \text{Normal}(0,0.2)\\
  \beta &\sim \text{Normal}(0,0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}


```{r}
m5.4 <- quap(
  alist(
    M ~ dnorm( mu , sigma ) , 
    mu <- a + bAM * A ,
    a ~ dnorm( 0 , 0.2 ) , 
    bAM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ) , data = d )

mu <- link(m5.4)
mu_mean <- apply( mu , 2 , mean ) 
mu_resid <- d$M - mu_mean
```

Brings home message that regression models measure the remaining association of each predictor with the outcome, after knowing the other predictors - computing the residuals shows this yourself, but with the unified model it happens automatically.


2. Posterior prediction plots - check fit and assess predictions, not causal tools

Just plot posterior against data, more easily diagnoses errors or why it could fail.

```{r}
# call link without specifying new data # so it uses original data
mu <- link( m5.3 )
# summarize samples across cases 
mu_mean <- apply( mu , 2 , mean ) 
mu_PI <- apply( mu , 2 , PI )
# simulate observations
# again no new data, so uses original data 
D_sim <- sim( m5.3 , n=1e4 )
D_PI <- apply( D_sim , 2 , PI )

plot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) , 
      xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 )
```

From this, understand that model under-predicts states with high divorce rates, and over-predicts those with low rates (89% CI shown). Several problematic ones, e.g. Idaho and Utah, high above the mean (high Latter Day Saints population with low divorce rate, can consider demographic compositions).


3. Counterfactual plots - explore causal implications of manipulating variables

Display implied predictions of the model - pick an intervention variable, define a range for it, simulate other values including outcome. Consider the DAG where $M$ affects $D$.

```{r}
data(WaffleDivorce) 
d <- list()
d$A <- standardize( WaffleDivorce$MedianAgeMarriage )
d$D <- standardize( WaffleDivorce$Divorce )
d$M <- standardize(WaffleDivorce$Marriage )
m5.3_A <- quap( 
  alist(
    ## A -> D <- M
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bM*M + bA*A , 
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    bA ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 ),
    ## A -> M
    M ~ dnorm( mu_M , sigma_M ), 
    mu_M <- aM + bAM*A,
    aM ~ dnorm( 0 , 0.2 ),
    bAM ~ dnorm( 0 , 0.5 ), 
    sigma_M ~ dexp( 1 )
) , data = d )
precis(m5.3_A)
```


This model shows $M$ and $A$ are strongly negatively associated, so manipulate $A$ for 30 observations around 2 sigma of the mean.

```{r}
A_seq <- seq( from=-2 , to=2 , length.out=30 )
# prep data
sim_dat <- data.frame( A=A_seq )
# simulate M and then D, using A_seq
s <- sim( m5.3_A , data=sim_dat , vars=c("M","D") )

# display counterfactual predictions
plot( sim_dat$A , colMeans(s$D) , ylim=c(-2,2) , type="l" ,
      xlab="manipulated A" , ylab="counterfactual D" ) 
shade( apply(s$D,2,PI) , sim_dat$A )
mtext( "Total counterfactual effect of A on D" )
```
```{r}
plot( sim_dat$A , colMeans(s$M) , ylim=c(-2,2) , type="l" ,
      xlab="manipulated A" , ylab="counterfactual M" ) 
shade( apply(s$M,2,PI) , sim_dat$A )
mtext( "Counterfactual effect of A -> M" )
```

The trick: when you manipulate a variable, you break the causal influence of other variables on it. For example, try breaking the causal chain from A->M.

```{r, fig.height=2}
dag_2 <- dagitty( "dag { A -> D <- M}")
coordinates(dag_2) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) ) 
drawdag( dag_2 )
```

```{r}
sim_dat <- data.frame( M=seq(from=-2,to=2,length.out=30) , A=0 ) 
s <- sim( m5.3_A , data=sim_dat , vars="D" )

plot( sim_dat$M , colMeans(s) , ylim=c(-2,2) , type="l" , 
      xlab="manipulated M" , ylab="counterfactual D" )
shade( apply(s,2,PI) , sim_dat$M )
mtext( "Total counterfactual effect of M on D" )
```


Only simulate $D$, not $A$ since $M$ doesn't influence it - notice the effect is not strong.


## 5.2 - Masked Relationship

Previous section showed multiple predictors are useful for dealing with spurious correlation. Next, look at measuring direct influences of multiple factors on an outcome when none is apparent from bivariate relationships - occurs when predictors are correlated, one positive, one negative.

New dataset - composition of milk across species.

```{r}
library(rethinking) 
data(milk)
d <- milk
str(d)
```

Hypothesis - larger brain -> more energetic milk. Look at $K$, calories per g of milk, $M$, body mass, and $N$, neocortex mass percent of the brain.

```{r}
d$K <- scale( d$kcal.per.g ) 
d$N <- scale( d$neocortex.perc ) 
d$M <- scale( log(d$mass) )
```

Model:

\begin{align*}
  K_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i = \alpha + \beta_N N_i
\end{align*}

Text asks us to try a model that will fail, giving an error that "initial value in 'vmmin' is not finite." This comes from NANs in neocortex size. We'll do drops now.

```{r}
 dcc <- d[ complete.cases(d$K,d$N,d$M) , ]
m5.5_draft <- quap( 
    alist(
      K ~ dnorm( mu , sigma ) , 
      mu <- a + bN*N ,
      a ~ dnorm( 0 , 1 ) ,
      bN ~ dnorm( 0 , 1 ) , 
      sigma ~ dexp( 1 )
) , data=dcc )
prior <- extract.prior( m5.5_draft )
xseq <- c(-2,2)
mu <- link( m5.5_draft , post=prior , data=list(N=xseq) )
plot( NULL , xlim=xseq , ylim=xseq ,  
      xlab="Neocortex % (std)", ylab="kCal per g (std)")
for ( i in 1:50 ) lines( xseq , mu[i,] , col=col.alpha("black",0.3) )
```


Crazy priors, let's tighten them.

```{r}
m5.5 <- quap( 
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bN*N ,
    a ~ dnorm( 0 , 0.2 ) , 
    bN ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data=dcc )
prior <- extract.prior( m5.5 )
xseq <- c(-2,2)
mu <- link( m5.5 , post=prior , data=list(N=xseq) )
plot( NULL , xlim=xseq , ylim=xseq , 
      xlab="Neocortex % (std)", ylab="kCal per g (std)")
for ( i in 1:50 ) lines( xseq , mu[i,] , col=col.alpha("black",0.3) )
```

Better, now looking at posterior:

```{r}
 precis( m5.5 )
```

Some things to takeaway: neither strong nor precise, std twice mean.

```{r}
xseq <- seq( from=min(dcc$N)-0.15 , to=max(dcc$N)+0.15 , length.out=30 ) 
mu <- link( m5.5 , data=list(N=xseq) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( K ~ N , data=dcc ,xlab="Neocortex % (std)", ylab="kCal per g (std)") 
lines( xseq , mu_mean , lwd=2 ) 
shade( mu_PI , xseq )
```

Also look at kCal v body mass independently

```{r}
m5.6 <- quap( 
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bM*M ,
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data=dcc ) 
precis(m5.6)
```


```{r}
xseq <- seq( from=min(dcc$M)-0.15 , to=max(dcc$M)+0.15 , length.out=30 ) 
mu <- link( m5.6 , data=list(M=xseq) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( K ~ M , data=dcc ,xlab="log body mass (std)", ylab="kCal per g (std)") 
lines( xseq , mu_mean , lwd=2 ) 
shade( mu_PI , xseq )
```


Multivariate model:

\begin{align*}
  K_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= \alpha + \beta_N N_i + \\
  \alpha &\sim \text{Normal}(0,0.2)\\
  \beta_n &\sim \text{Normal}(0,0.5)\\
  \beta_m &\sim \text{Normal}(0,0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

```{r}
m5.7 <- quap( 
  alist(
    K ~ dnorm( mu , sigma ) ,
    mu <- a + bN*N + bM*M , 
    a ~ dnorm( 0 , 0.2 ) , 
    bN ~ dnorm( 0 , 0.5 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data=dcc ) 
precis(m5.7)
```

Posterior of both with the outcome increased.

```{r} 
plot( coeftab( m5.5 , m5.6 , m5.7 ) , pars=c("bM","bN") )
```

(Top is bM, bottom is bN)

Mean for bM, the neocortex percent, increased by a factor of 5 (bottom), the mean for the log body mass is now much larger too. 

```{r}
pairs( ~K + M + N, dcc, col=col.alpha("blue",0.7))
```

We can see that M and N are positively correlated to each other, so they cancel out. Multiple regression is helpful - asking if a species has a high neocortex percent _for their body mass_; similarly if they have a high body mass _for their neocortex percent_.

Three DAGs we'll explore:

```{r, fig.height=2}
dag1 <- dagitty( "dag {
  M -> N
  M -> K
  N -> K
}")
coordinates(dag1) <- list( x=c(M=0,K=1,N=2) , y=c(M=0,K=1,N=0) ) 
drawdag( dag1 )
```

Body mass influences neocortex percent, both influence calories

```{r, fig.height=2}
dag2 <- dagitty( "dag {
  N -> M
  M -> K
  N -> K
}")
coordinates(dag2) <- list( x=c(M=0,K=1,N=2) , y=c(M=0,K=1,N=0) ) 
drawdag( dag2 )
```

Neocortex percent influences body mass, both influence calories.

```{r, fig.height=2}
dag3 <- dagitty( "dag {
  M <- U -> N
  M -> K
  N -> K
}")
coordinates(dag3) <- list( x=c(M=0,K=1,U=1,N=2) , y=c(M=0,K=1,U=0,N=0) ) 
drawdag( dag3 )
```


Some unobserved variable $U$ influences both $M$ and $N$ - unobserved variables will be explored more in next chapter. 

Can't tell which of the 3 is correct because the _conditional dependencies_ are the same between them; each suggests all variables are associated regardless of what we condition on - this is known as a "Markov Equivalence set."

Last some counterfactual plots:

```{r}
# Plot 1
xseq <- seq( from=min(dcc$M)-0.15 , to=max(dcc$M)+0.15 , length.out=30 ) 
mu <- link( m5.7 , data=data.frame( M=xseq , N=0 ) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)

par(mfrow=c(1,2))
plot( NULL , xlim=range(dcc$M) , ylim=range(dcc$K),
      xlab="log body mass (std)", ylab="kCal per g (std)", main="Counterfactual N=0") 
lines( xseq , mu_mean , lwd=2)
shade( mu_PI , xseq )

#plot2
xseq <- seq( from=min(dcc$N)-0.15 , to=max(dcc$N)+0.15 , length.out=30 ) 
mu <- link( m5.7 , data=data.frame( N=xseq , M=0 ) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)

plot( NULL , xlim=range(dcc$M) , ylim=range(dcc$K),
            xlab="Neocortex % (std)", ylab="kCal per g (std)", main="Counterfactual M=0")
lines( xseq , mu_mean , lwd=2 )
shade( mu_PI , xseq )

```


Overthinking section - simulating DAGs

```{r}
# M -> K <- N
# M -> N
n <- 100
M <- rnorm( n )
N <- rnorm( n , M )
K <- rnorm( n , N - M )
d_sim <- data.frame(K=K,N=N,M=M)

# M -> K <- N
# N -> M
n <- 100
N <- rnorm( n )
M <- rnorm( n , N )
K <- rnorm( n , N-M )
d_sim2 <- data.frame(K=K,N=N,M=M)

# M -> K <- N
# M <- U -> N
n <- 100
U <- rnorm( n )
N <- rnorm( n , U )
M <- rnorm( n , U )
K <- rnorm( n , N-M)
d_sim3 <- data.frame(K=K,N=N,M=M)
```

This code simulates data from all 3 dags into dataframes

```{r}
dag5.7 <- dagitty( "dag{ 
  M -> K <- N
  M -> N }" )
coordinates(dag5.7) <- list( x=c(M=0,K=1,N=2) , y=c(M=0.5,K=1,N=0.5) ) 
MElist <- equivalentDAGs(dag5.7)
drawdag(MElist)
```

All possible Markov Equivalent dags.

## 5.3 - Categorical Variables

Also known as _factors_, can be put into linear models.

#### Binary categories

Male/female example

```{r}
data(Howell1) 
d <- Howell1 
str(d)
```

"male" is an _indicator_ or dummy variable, indicates the category. Make a model focused on sex, where $m$ is male indicator.


\begin{align*}
  h_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= \alpha + \beta_m m_i \\
  \alpha &\sim \text{Normal}(178,20)\\
  \beta_m &\sim \text{Normal}(0,10)\\
  \sigma &\sim \text{Uniform}(0,50)
\end{align*}


So for males you get a linear model, $\mu_i = \alpha + \beta_m m_i$ and for females you get just $\mu_i = \alpha$. This makes $\beta_m$ the expected difference between males and females; $\alpha$ no longer interpreted as average sample height but average female height.

```{r}
mu_female <- rnorm(1e4,178,20)
mu_male <- rnorm(1e4,178,20) + rnorm(1e4,0,10) 
#precis( data.frame( mu_female , mu_male ) )
```

Male prior wider than female, but we're not less certain, so use an index variable instead

```{r}
d$sex <- ifelse( d$male==1 , 2 , 1 ) 
str( d$sex )
```

for 1 female, 2 male, making the model:

\begin{align*}
  h_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= \alpha_{\text{SEX}[i]}\\
  \alpha_j &\sim \text{Normal}(178,20), \text{for } j=1..2\\
  \sigma &\sim \text{Uniform}(0,50)
\end{align*}


Making the model:

```{r}
m5.8 <- quap( 
  alist(
    height ~ dnorm( mu , sigma ) , 
    mu <- a[sex] ,
    a[sex] ~ dnorm( 178 , 20 ) , 
    sigma ~ dunif( 0 , 50 )
  ) , data=d ) 
precis( m5.8 , depth=2 ) #depth=2 -> show vectors
```

```{r}
post <- extract.samples(m5.8)
post$diff_fm <- post$a[,1] - post$a[,2] 
#precis( post , depth=2)
```

precis output will have a diff_fm row - _contrast_.


#### Many categories

Going back to milk example:

```{r}
data(milk)
d <- milk 
unique(d$clade)
```

auto-index categories

```{r}
d$clade_id <- as.integer( d$clade)
```

Model:

\begin{align*}
  K_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= \alpha_{\text{CLADE}[i]}\\
  \alpha_j &\sim \text{Normal}(0,0.5), \text{for } j=1..4\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

```{r}
d$K <- scale( d$kcal.per.g ) 
m5.9 <- quap(
  alist(
    K ~ dnorm( mu , sigma ),
    mu <- a[clade_id],
    a[clade_id] ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
  ) , data=d )
labels <- paste( "a[" , 1:4 , "]:" , levels(d$clade) , sep="" )
plot( precis( m5.9 , depth=2 , pars="a" ) , labels=labels ,
    xlab="expected kcal (std)" )
```

Rethinking block - common error in interpretation of parameter estimates is that because a parameter is far from zero and another isn't (is/isn't significant) that the difference is significant. If you want to know the distribution of the difference, calculate the contrast between the two parameters, not investigate independently.


## 5.4 - Summary 

Multiple regression can construct models with more than one predictor - letting us answer the question "what is the value of knowing each predictor, once others are known."

1. Focuses on value of predictors as a description, not a forecast only
2. Assumption that value of predictors do not depend on values of other predictors (later confronted)

# Chapter 4 - Geocentric Models
```{r, include=FALSE}
library(rethinking)
```


Ptolemy came up with an extremely accurate geocentric model of our solar system in 1st century, and used for over 1000 years. Based on epicycle - circles on circles, which generalize to a Fourier series with enough circles. Comparison to linear regression - don't want to read too literally, but useful.

## 4.1 Why normal distributions are normal

#### Normal by Addition

Introduce from a random walk perspective:
```{r, fig.height=3}
pos <- replicate( 10000 , sum( runif(16,-1,1) ) )
dens(pos,  norm.comp=TRUE)
```

Gaussian structure ultimately emerges - processes that add together random values result in Gaussians.

#### Normal by Multiplication

Consider an organism with growth rate influenced by local interactions; multiplicative effect. Sample growth rates between 1.0 and 1.1:

```{r}
growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) )
dens( growth , norm.comp=TRUE )
```


#### Normal by log-multiplication

Large deviates multiplied don't produce Gaussians, but do produce Gaussians at log scale

```{r}
log.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )
dens(log.big, norm.comp=TRUE )
```


This is because adding logs is the equivalent to multiplying original numbers.

#### Using Gaussian Distributions

Rest of chapter will be using Gaussians as a skeleton; 2 justifications

- Ontological - World is full of approximate Gaussians. Gaussian is a member of the "exponential family," all of which are important for science and seen in the natural world.

- Epistemological - Represents a state of ignorance: all we know or are willing to say is mean and variance. Premised on Information theory and maximum entropy (chapters 7 and 10).

## 4.2 - A language for describing models

1. Recognize a set of variables to understand. Observable ones are data, unobservable are parameters.
2. For each variable define in terms of other variables or a probability distribution.
3. The combination of variables and probability distributions defines a _joint generative model_.

Then summarize the model, e.g.

\begin{align*}
  y_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= \beta x_i\\
  \beta &\sim \text{Normal}(0, 10)\\
  \sigma &\sim  \text{Exponential}(1)\\
  x_i &\sim \text{Normal}(0,1)
\end{align*}

Immense flexibility with this - don't need to worry about conditions like homoscedasticity since in the model definition, natural ways to change assumptions so not stuck in fixed model type.

#### Re-describing globe tossing

\begin{align*}
  W &\sim \text{Binomial}(N,p)\\
  p &\sim \text{Uniform}(0,1)
\end{align*}

Said: "Count W is distributed binomially, sample size $N$, probability $p$. Prior for $p$ is uniform between 0 and 1.

Both are stochastic, which is indicated by $\sim$, and implies not known with certainty, just probabilistic.

## 4.3 - Gaussian model of height

Building a linear regression model, consider all Gaussians, so posterior is a distribution of Gaussians distributions


#### Data 

1960s partial census data from foraging population of !Kung San

```{r}
library(rethinking)
data(Howell1)
d<- Howell1 #dataframe object
d2 <- d[ d$age >=18 , ] # Create new, just adult one
# precis(d2)  ### run this - but doesn't compile on r-markdown
```


#### The model 

Adult heights from a single population are approximately normal, so model using Gaussian, but which one? Model is:

\begin{align*}
h_i &\sim \text{Normal}(\mu,\sigma)\\
\mu &\sim \text{Normal}(178, 20)\\
\sigma &\sim \text{Uniform}(0,50)
\end{align*}

Estimate on mean of $\mu$ comes from author height being 178 cm, so $178 \pm 40$ seems reasonable. Sigma a 0 because standard deviations need to be positive. Plotting priors:

```{r}
par(mfrow=c(1,2))
curve( dnorm( x , 178 , 20 ) , from=100 , to=250, xlab=expression(mu))
curve( dunif( x , 0 , 50 ) , from=-10 , to=60, xlab=expression(sigma))
```

**Prior predictive** simulation should be run, to see implications of prior choices.

```{r}
sample_mu <- rnorm( 1e4 , 178 , 20 )
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e5 , sample_mu , sample_sigma ) 
dens( prior_h )
```

You could consider a less informative prior, e.g. $\mu \sim \text{Normal}(178,100)$, but this will give you odd things like negative heights, or people taller than the record tallest person - this is why these tests are useful.

#### Grid approximation of posterior

Worth running posterior since first multi-parameter model and first Gaussian. Code given without explanation :)

```{r}
mu.list <- seq( from=150, to=160 , length.out=100 ) 
sigma.list <- seq( from=7 , to=9 , length.out=100 ) 
post <- expand.grid( mu=mu.list , sigma=sigma.list ) 
post$LL <- sapply( 1:nrow(post) , function(i) sum(
  dnorm( d2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) ) 
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
    dunif( post$sigma , 0 , 50 , TRUE ) 
post$prob <- exp( post$prod - max(post$prod) )
contour_xyz( post$mu , post$sigma , post$prob,
            xlab=expression(paste("Sample ",mu)), 
            ylab=expression(paste("Sample ",sigma)))
```

```{r}
 image_xyz( post$mu , post$sigma , post$prob , 
            xlab=expression(paste("Sample ",mu)), 
            ylab=expression(paste("Sample ",sigma)))
```

Sampling posterior

```{r}
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE , prob=post$prob )
sample.mu <- post$mu[ sample.rows ] 
sample.sigma <- post$sigma[ sample.rows ]
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
```

(note - the text has a different plot for this, theirs has no horizontal spaces, oddly)


```{r}
par(mfrow=c(1,2))
dens( sample.mu , main=expression(mu)) 
dens( sample.sigma, main=expression(sigma) )
```

Here we can see something has gone odd with the posterior $\mu$. Regardless, this is sampling of the _marginal_ posterior density, meaning averaging over other parameters.

#### Finding the posterior with quap

Moving onto quadratic approximation (repeating some code for clarity).

```{r}
# Load Data
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ] # Select Adults

flist <- alist(
  height ~ dnorm( mu , sigma ), # height ~ Normal(mu,sigma)
  mu ~ dnorm( 178 , 20 ),  # mu ~ Normal (178, 20)
  sigma ~ dunif( 0 , 50 ) #sigma ~ Uniform(0,50)
)

# Fit the model to the data in the frame
start <- list( # Give nice starting location
  mu=mean(d2$height),
  sigma=sd(d2$height)
)
m4.1 <- quap( flist , data=d2 , start=start)
# Look at posterior
precis( m4.1 , prob=0.95)
```

This shows the marginal distribution of each parameter - 

#### Sampling from a quap

When R constructs quadratic, it calculates covariances, which are sufficient to make Gaussians. Variance-covariance matrix given by:

```{r}
vcov( m4.1 )
```
Decomposition:
```{r}
diag( vcov( m4.1 ) ) 
cov2cor( vcov( m4.1 ) )
```

Sample vectors from of values from multi-dimensional Gaussian distribution (provided by rethinking package) 

```{r}
post <- extract.samples( m4.1 , n=1e4 ) 
#precis(post)
```


## 4.4 - Linear Prediction

Typically interested in how outcome is related to a _predictor variable_. Plot height and weight to see how well they covary.

```{r}
plot( d2$height ~ d2$weight )
```

#### Linear Model Strategy 

Strategy - make parameter for the mean of a Gaussian $\mu$ into a linear function of the predictor variable and other new parameters we invent. Instructs the analysis to assume that the predictor has a **constant** and **additive** relationship to the mean of the outcome.

Basically - "consider all lines that relate one variable to the other, rank in order of plausibility given the data"

Before our Gaussian model was:
\begin{align*}
  h_i &\sim \text{Normal}(\mu,\sigma)\\
  \mu &\sim \text{Normal}(178,20)\\
  \sigma &\sim \text{Uniform}(0,50)
\end{align*}


To get weight into the Gaussian model we instead relate the two, letting $x$ be the column of weight measurements, with average $\bar{x}$

\begin{align*}
  h_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= \alpha + \beta(x_i-\bar{x})\\
  \alpha &\sim \text{Normal}(178,20)\\
  \beta &\sim \text{Normal}(0,10)\\
  \alpha &\sim \text{Uniform}(0,50)
\end{align*}


Step by step:

$h_i \sim \text{Normal}(\mu_i,\sigma)$, the probability of the observed height, this is basically unchanged, except $\mu$ has been replaced by $\mu_i$, implying mean depends on row.

$\mu_i = \alpha + \beta(x_i-\bar{x})$, the linear model. No longer estimate $\mu$, construct from other parameters. Notice, not stochastic ($=$, not $\sim$), meaning this is deterministic; once we know $\alpha$ and $\beta$ and $x_i$, we know $mu_i$. $alpha$ and $\beta$ are made up, just devices for manipulating $\mu$; think of them as targets of learning.

The rest are priors. All have been seen before except $\beta$. If we look at many values of $\beta$:

```{r,include=FALSE}
#par(mfrow=c(1,2))
set.seed(2971)
N <- 100
a <- rnorm( N , 178 , 20 ) 
b <- rnorm( N , 0 , 10 )
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) , 
                        from=min(d2$weight) , 
                        to=max(d2$weight) ,
                        add=TRUE , 
                        col=col.alpha("black",0.2) )
```

We can immediately see that this is barely constrained, even negative values. Try swapping it for something more useful, $\beta \sim \text{Log-Normal}(0,1)$

```{r,include=FALSE}
b <- rlnorm( 1e4 , 0 , 1 )
dens( b , xlim=c(0,5) , adj=0.1 )
set.seed(2971)
N <- 100 # 100 lines a <- rnorm( N , 178 , 20 )
b <- rlnorm( N , 0 , 1 )
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "log(b) ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) , 
                        from=min(d2$weight) , 
                        to=max(d2$weight) ,
                        add=TRUE , 
                        col=col.alpha("black",0.2) )
```

Much better constraints.

#### Finding the posterior

Let's define full model:

```{r}
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]
# define the average weight, x-bar 
xbar <- mean(d2$weight)
# fit model 
m4.3 <- quap(
alist(
  height ~ dnorm( mu , sigma ) , 
  mu <- a + b*( weight - xbar ) , 
  a ~ dnorm( 178 , 20 ) ,
  b ~ dlnorm( 0 , 1 ) ,
  sigma ~ dunif( 0 , 50 ) ),
data=d2 )
```


### Interpreting the posterior 

Table approach:
```{r}
precis(m4.3, prob=.95)
```

The new parameter, $\beta$, is a slope - interpret as "a person 1 kg heavier is expected to be .9 cm taller." 95% of the posterior is between 0.82 and 0.99, so values around 0 or higher than 1 are highly incompatible with these data and model. This is _not_ evidence the relationship is linear, but saying "if you commit to lines, ones around 0.9 slope are reasonable."

```{r}
 round( vcov( m4.3 ) , 3 )
```
```{r}
pairs(m4.3)
```
Low covariation.

#### Plotting posterior inference against the data 

Always useful to plot posterior over data to check fit worked, and to interpret posterior.

```{r}
plot( height ~ weight , data=d2 , col=rangi2 ) 
post <- extract.samples( m4.3 )
a_map <- mean(post$a)
b_map <- mean(post$b)
curve( a_map + b_map*(x - xbar) , add=TRUE )
```

#### Adding uncertainty around mean

While that's the most plausible line, many others are possible, looking at these are good for communicating uncertainty.

```{r}
N <- 350
dN <- d2[ 1:N , ] 
mN <- quap(
  alist(
  height ~ dnorm( mu , sigma ) ,
  mu <- a + b*( weight - mean(weight) ) , 
  a ~ dnorm( 178 , 20 ) ,
  b ~ dlnorm( 0 , 1 ) ,
  sigma ~ dunif( 0 , 50 )
  ) , data=dN )

# extract 20 samples from the posterior 
post <- extract.samples( mN , n=20 )
# display raw data and sample size
plot( dN$weight , dN$height ,
    xlim=range(d2$weight) , ylim=range(d2$height) ,
    col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N))
# plot the lines, with transparency
for ( i in 1:50 )
  curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) , 
         col=col.alpha("black",0.3) , add=TRUE )
```

For 350 points, 50 possible lines.

#### Plotting regression intervals and contours

For a single value (e.g. 50 kg), we can make a distribution

```{r}
post <- extract.samples( m4.3 )
mu_at_50 <- post$a + post$b * ( 50 - xbar )
dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )
```

If we want to evaluate over all the values, we can use the link function, which generates posterior samples (must pass it data to evaluate on). Then can use points to visualize.

```{r}
# define sequence of weights to compute predictions for # these values will be on the horizontal axis 
weight.seq <- seq( from=25 , to=70 , by=.5 )
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link( m4.3 , data=data.frame(weight=weight.seq) ) 

# use type="n" to hide raw data
plot( height ~ weight , d2 , type="n" )
# loop over samples and plot each mu value
for ( i in 1:100 )
points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )
```

Alternatively, shade function works too.

```{r}
# summarize the distribution of mu 
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )

# plot raw data
# fading out points to make line and interval more visible 
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight 
lines( weight.seq , mu.mean )
# plot a shaded region for 89% PI 
shade( mu.PI , weight.seq )
```


#### Prediction Intervals

Important - we've only considered the range of $\mu$, not included $\sigma$. We can do that via the sim function, which simulates heights. Then we can plot all the distributions we've seen so far, the data, the MAP, the PI, and the HPDI.

```{r}
sim.height <- sim( m4.3 , data=list(weight=weight.seq) )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

# plot raw data
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.6) )
# draw MAP line
lines( weight.seq , mu.mean ,col="red")
# draw HPDI region for line 
mu.HPDI <- apply( mu , 2 , HPDI , prob=0.89)
shade( mu.HPDI , weight.seq, col=col.alpha('cornflowerblue',0.4) )
# draw PI region for simulated heights 
shade( height.PI , weight.seq , col=col.alpha('cornflowerblue',0.2) )
```

## Curves From Lines

No a priori reason why linear models are special, just that they're simple. Can consider a polynomial model:

\begin{align*}
  h_i &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= \alpha +\beta_1 x_i + \beta_2 x_i^2\\
  \alpha &\sim \text{Normal}(178,20)\\
  \beta_1 &\sim \text{Log-Normal}(0,1)\\
  \beta_2 &\sim \text{Normal}(0,1)\\
  \sigma &\sim \text{Uniform}(0,50)
\end{align*}


```{r}
library(rethinking) 
data(Howell1)
d <- Howell1

d$weight_s <- ( d$weight - mean(d$weight) )/sd(d$weight) 
d$weight_s2 <- d$weight_s^2
m4.5 <- quap(
  alist(
  height ~ dnorm( mu , sigma ) ,
  mu <- a + b1*weight_s + b2*weight_s2 , a ~ dnorm( 178 , 20 ) ,
  b1 ~ dlnorm( 0 , 1 ) ,
  b2 ~ dnorm( 0 , 1 ) ,
  sigma ~ dunif( 0 , 50 )
), 
data=d )
precis( m4.5 )
```




```{r}
weight.seq <- seq( from=-2.2 , to=2 , length.out=30 )
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 ) 
mu <- link( m4.5 , data=pred_dat )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( m4.5 , data=pred_dat )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) ) 
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )
```

Can consider even higher orders (cubic, quartic, etc.), but just because they fit the sample better doesn't make them better models, moreover it doesn't have biological information, so no causal relationships can be found. These are addressed later.

Next can also consider spline models, looking at the cherry blossom dataset.

```{r}
library(rethinking) 
data(cherry_blossoms) 
d <- cherry_blossoms 
#precis(d)
```

Splines basically split up a predictor into parts, then has weights for when they turn on/turn off:

$$
  \mu_i = \alpha + w_1 B_{i,1} + w_2 B_{i,2} + w_3 B_{i,3} + \ldots
$$

Pivot points are called knots. Here we make a model with 15 knots, which are of degree 3.

```{r}
d2 <- d[ complete.cases(d$temp) , ] # complete cases on temp 
num_knots <- 15
knot_list <- quantile( d2$year , probs=seq(0,1,length.out=num_knots) )

library(splines) 
B <- bs(d2$year,
        knots=knot_list[-c(1,num_knots)] , 
        degree=3 , intercept=TRUE )

plot( NULL , xlim=range(d2$year) , ylim=c(0,1) , xlab="year" , ylab="basis value" )
for ( i in 1:ncol(B) ) lines( d2$year , B[,i])
```

Next we fit to data and plot the knots times their weight values

```{r}
m4.7 <- quap( 
  alist(
    T ~ dnorm( mu , sigma ) , 
    mu <- a + B %*% w ,
    a ~ dnorm(6,10),
    w ~ dnorm(0,1),
    sigma ~ dexp(1) 
    ),
    data=list( T=d2$temp , B=B ) ,
    start=list( w=rep( 0 , ncol(B) ) ) )

post <- extract.samples(m4.7)
w <- apply( post$w , 2 , mean )
plot( NULL , xlim=range(d2$year) , ylim=c(-2,2) ,
    xlab="year" , ylab="basis * weight" )
for ( i in 1:ncol(B) ) lines( d2$year , w[i]*B[,i] )
```

Last we can fit the full splined model to the data

```{r}
mu <- link( m4.7 )
mu_PI <- apply(mu,2,PI,0.97)
plot( d2$year , d2$temp , col=col.alpha(rangi2,0.3) , pch=16 ) 
shade( mu_PI , d2$year , col=col.alpha("black",0.5) )
```

This model is:

\begin{align*}
  T_i &\sim \text{Normal}(\mu_i,\sigma) \\
  \mu_i &= \alpha + \sum_{k=1}^{K} w_kB_{k,i} \\
  \alpha &\sim \text{Normal}(6,10) \\
  w_j &\sim \text{Normal}(0,1) \\
  \sigma &\sim \text{Exponential}(1)
\end{align*}


## Summary

Looked a linear regression, estimating association between a predictor and outcome. The likelihood is comprised of a Gaussian.


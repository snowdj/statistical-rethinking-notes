# Chapter 6 - The Haunted DAG & The Causal Terror
```{r, include=FALSE}
library(rethinking)
```

Take two uncorrelated variables (newsworthiness and trustworthiness), select the top 10%, and draw a linear regression and there will be be a negative correlation - _Berkson's Paradox_ or _selection-distortion_ effect.

This can happen within multiple regression, if adding a predictor induces selection, known as _collider bias_.

In this chapter - warnings about adding random variables without an idea of a causal model: multicollinearity, post-treatment bias, collider bias.

## 6.1 Multicollinearity

"Very strong correlation between two or more predictor variables"

Technically nothing wrong with it, will work well for prediction, just hard to understand.

Return to primate milk data for data.


### Multicollinear legs

Using both leg lengths (left and right) as a predictor

```{r}
# Generate dataframe
N <- 100
set.seed(909)
height <- rnorm(N,10,2) 
leg_prop <- runif(N,0.4,0.5) 
leg_left <- leg_prop*height +
  rnorm( N , 0 , 0.02 ) 
leg_right <- leg_prop*height +
  rnorm( N , 0 , 0.02 )
d <- data.frame(height,leg_left,leg_right)

# Make model
m6.1 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + bl*leg_left + br*leg_right , 
    a ~ dnorm( 10 , 100 ) ,
    bl ~ dnorm( 2 , 10 ) ,
    br ~ dnorm( 2 , 10 ) ,
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.1)
```

```{r}
plot(precis(m6.1))
```

Gigantic means and standard deviations. Question posed in last chapter is "what's the value of knowing a second leg length after knowing the first?" 

```{r}
post <- extract.samples(m6.1)
plot( bl ~ br , post , col=col.alpha(rangi2,0.1) , pch=16 )
```

Posterior for left vs right very highly correlated - as $\beta_l$ is large, $\beta_r$ is small, they carry the same variation.

Our model development:

\begin{align*}
  y_i &\sim \text{Normal}(\mu,\sigma)\\
  \mu_i &= \alpha + \beta_1 x_l + \beta_2 x_r\\
\end{align*}

But due to covariation (x is basically the same thing), the computer basically sees:

\begin{align*}
  y_i &\sim \text{Normal}(\mu,\sigma)\\
  \mu_i &= \alpha + (\beta_1 +\beta_2)  x_i\\
\end{align*}

As in only the sum of $\beta_1 + \beta_2$ influences $\mu$.

```{r}
sum_blbr <- post$bl + post$br
dens( sum_blbr , col=rangi2 , lwd=2 , xlab="sum of bl and br" )
```


Using just one predictor:

```{r}
m6.2 <- quap( 
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + bl*leg_left,
    a ~ dnorm( 10 , 100 ) ,
    bl ~ dnorm( 2 , 10 ) ,
    sigma ~ dexp( 1 ) 
  ),
  data=d ) 
precis(m6.2)
```

Which is basically identical to the other model - when the two predictor variables are strongly correlated, including both may lead to confusion.

### Multicollinear milk

Legs example is contrived, something more elaborate

```{r}
data(milk)
d <- milk
d$K <- scale( d$kcal.per.g ) 
d$F <- scale( d$perc.fat ) 
d$L <- scale( d$perc.lactose )
```

Looking at bivariate regressions:


```{r}
# kcal.per.g regressed on perc.fat 
m6.3 <- quap(
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bF*F ,
    a ~ dnorm( 0 , 0.2 ) , 
    bF ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ) , data=d )

# kcal.per.g regressed on perc.lactose 
m6.4 <- quap(
  alist(
    K ~ dnorm( mu , sigma ) ,
    mu <- a + bL*L ,
    a ~ dnorm( 0 , 0.2 ) , 
    bL ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ) , data=d )

m6.5 <- quap(
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bF*F + bL*L , 
    a ~ dnorm( 0 , 0.2 ) , 
    bF ~ dnorm( 0 , 0.5 ) , 
    bL ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
  ),data=d ) 

precis( m6.3 ) 
precis( m6.4 )
precis( m6.5 )
```



Percent fat (perc.fat) and percent lactose (perc.lactose) are mirrors of each other (m6.3 and 6.4). Putting both in a model (6.5) gives posterior means of both closer to 0 than either individually - mutual information.

```{r}
pairs( ~ kcal.per.g + perc.fat + perc.lactose , data=d , col=rangi2 )
```

Very obvious from pairs plot, either helps, but neither does if you already know the other. Causal approach is the best approach.

```{r, fig.height=2}
dag0 <- dagitty( "dag {
  D [unobserved]
  L <- D -> F
  L -> K
  F -> K
}")
coordinates(dag0) <- list( x=c(L=0,D=1,K=1,F=2) , y=c(L=0,D=0,K=1,F=0) ) 
drawdag( dag0 )
```


Central trade-off is on $D$, the density of milk (non-observed) - fat and lactose are from that (mediators). 

_Non-Identifiability_ - structure of data and model do not make it possible to estimate a parameter's value. Can be coding, more often though just nature making it difficult.

## 6.2 Post-treatment Bias

_Omitted Variable Bias_ - Mistaken inferences that arise from omitting predictor variables

_Post-treatment Bias_ - Mistaken inference that arise from including variables that are the consequences of others. 

Name "post-treatment" stems from experimental design, adding variables to model that are a result of the experiment, not truly independent. E.g. plant growing experiment, outcome of interest is final height but some plants grow fungus during the experiment - should not include this.

```{r}
set.seed(71)
# number of plants 
N <- 100
# simulate initial heights 
h0 <- rnorm(N,10,2)
# assign treatments and simulate fungus and growth 
treatment <- rep( 0:1 , each=N/2 )
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 ) 
h1 <- h0 + rnorm(N, 5 - 3*fungus)
# compose a clean data frame
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus ) 
#precis(d)
```

### A prior is born

Best approach is to pretend you don't have the data generating process.

For this example, height at $t=1$ is height than that at $t=0$, so put parameters on a scale of _proportion_ of initial height - allows easier prior setting.

\begin{align*}
  h_{1,i} &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= h_{0,i} \times p \\
\end{align*}

So $p=2$ means at $t=1$ it doubles in height, $p=1$ means same height. Set this prior to 1, to allow for negative growth, force $p>0$ since a proportion - use Log-Normal since it's always positive.

```{r}
sim_p <- rlnorm( 1e4 , 0 , 0.25 ) 
#precis( data.frame(sim_p) )
```

Covers everything from 40% shrinking to 50% growth, centered at 1.

```{r}
m6.6 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ), 
    mu <- h0*p,
    p ~ dlnorm( 0 , 0.25 ), 
    sigma ~ dexp( 1 )
), data=d ) 
precis(m6.6)
```
On average, about 40% growth.

Now the model we said would be bad before - adding fungus growth

\begin{align*}
  h_{1,i} &\sim \text{Normal}(\mu_i,\sigma)\\
  \mu_i &= h_{0,i} \times p \\
  p &= \alpha + \beta_T T_i + \beta_F F_i \\
  \alpha &\sim \text{Log-Normal}(0,0.25)\\
  \beta_T &\sim \text{Normal}(0,0.5)\\
  \beta_F &\sim \text{Normal}(0,0.5)\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

Sample model, priors on slopes may be too flat (between -1 and +1), but otherwise proportion of growth defined.

```{r}
m6.7 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment + bf*fungus, 
    a ~ dlnorm( 0 , 0.2 ) ,
    bt ~ dnorm( 0 , 0.5 ),
    bf ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.7)
```

Nearly same posterior. $\beta_T$  is 0, tight interval - not associated. $\beta_F$ is negative - hurts growth. What happened?

### Blocked by consequence

Fungus is a consequence of treatment - "post-treatment variable." Controlling for fungus, model answers the question: "Once we already know if a plant developed fungus, does soil treatment matter?" Which, no, it's 0. But we care about treatment.

```{r}
m6.8 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment,
    a ~ dlnorm( 0 , 0.2 ), 
    bt ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.8)
```

Now the model is a lot more sensible 

- Makes sense to control for pre-treatment effects (e.g. initial height) as they might mask causal influence
- Post-treatment inclusion can mask the treatment itself

### Fungus and _d_-separation

Draw the DAG:

```{r, fig.height=2}
plant_dag <- dagitty( "dag {
  H_0 -> H_1 
  F -> H_1 
  T -> F
}")
coordinates( plant_dag ) <- list( x=c(H_0=0,T=2,F=1.5,H_1=1) ,
                                  y=c(H_0=0,T=0,F=0,H_1=0) )
drawdag( plant_dag )
```

$T$ influences $F$ which influences $H_1$. By including $F$, you "block" $T$, or another way to say this is that conditioning on $F$ induces _d-separation_ (d for directional). This means that some variables on a directed graph are independent of others, no path connecting them. Going back to _conditional independencies:_

```{r}
impliedConditionalIndependencies(plant_dag)
```

The last one is important - our outcome is independent of $T$, given conditioning on $F$.

```{r, fig.height=2}
moisture_dag <- dagitty( "dag {
  M [unobserved]
  H_0 -> H_1 
  H_1 <- M -> F
  F -> T
}")
coordinates( moisture_dag ) <- list( x=c(H_0=0,T=2,F=1.5,H_1=0.5,M=1) ,
                                     y=c(H_0=0,T=0,F=0,H_1=0,M=0.5) )
drawdag( moisture_dag )
```

Another DAG - Now, an unobserved moisture variable affects both height and fungus growth. In this case (with hypothetical plant unaffected by a fungus), $H1$ and $T$ will appear to be not associated, but then the conditioning on $F$ suddenly fools you into thinking there's an association.

```{r}
set.seed(71)
N <- 1000
h0 <- rnorm(N,10,2)
treatment <- rep( 0:1 , each=N/2 )
M <- rbern(N)
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 + 0.4*M )
h1 <- h0 + rnorm( N , 5 + 3*M )
d2 <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )

m6.7_2 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment + bf*fungus, 
    a ~ dlnorm( 0 , 0.2 ) ,
    bt ~ dnorm( 0 , 0.5 ),
    bf ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp( 1 )
  ), data=d2 ) 
precis(m6.7_2)

m6.8_2 <- quap( 
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment,
    a ~ dlnorm( 0 , 0.2 ), 
    bt ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
  ), data=d2 ) 
precis(m6.8_2)
```

Next section fleshes this effect out more.

## 6.3 Collider Bias

The initial claim in the chapter of negative correlation between newsworthiness and trustworthiness resulting from selection process is a type of _collider bias_.

```{r, fig.height=2}
collider_dag <- dagitty( "dag {
  T -> S <- N
}")
coordinates( collider_dag ) <- list( x=c(T=0,S=1,N=2) ,
                                     y=c(T=0,S=0,N=0) )
drawdag( collider_dag )
```

A "collider" is when 2 arrows enter, as in $S$ for that DAG. Conditioning on colliders have statistical, not causal, associations between causes - learning a proposal was selected for trustworthiness provides information about newsworthiness if you know a proposal was selected (if low $T$, high $N$).

### Collider of false sorrow

Turn to problem of happiness and age. Say happiness, $H$, is a fixed parameter at birth, but influences events - happier is more likely to get married $M$. Age, $A$, is also correlated to marriage (older = more likely), so get same collider dag:

```{r, fig.height=2}
collider_dag <- dagitty( "dag {
  H -> M <- A
}")
coordinates( collider_dag ) <- list( x=c(T=0,S=1,N=2) ,
                                     y=c(T=0,S=0,N=0) )
drawdag( collider_dag )
```

Even though no causal association between happiness and age, if conditioned on marriage, induces an association.

```{r}
library(rethinking)
d <- sim_happiness( seed=1977 , N_years=1000 ) 
#precis(d)
```

Simulation of that data: 1300 people from 0-65. Want to ask if age is related to happiness - try multiple regression with linear model:

\begin{align*}
  \mu_i = \alpha_{\text{MID}[i]} + \beta_A A_i
\end{align*}

MID here is marriage status, 1 is single and 2 is married. Only want older than 18 years old, so correct it 0 to 1, 0 is 18 and 1 is 65. Happiness is arbitrary between -2 and 2.

```{r}
d2 <- d[ d$age>17 , ] # only adults 
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )

d2$mid <- d2$married + 1 
m6.9 <- quap(
  alist(
    happiness ~ dnorm( mu , sigma ), 
    mu <- a[mid] + bA*A,
    a[mid] ~ dnorm( 0 , 1 ),
    bA ~ dnorm( 0 , 2 ),
    sigma ~ dexp(1)
  ) , data=d2 ) 
precis(m6.9,depth=2)
```

This model says age is negatively associated with happiness - try omitting marriage

```{r}
m6.10 <- quap( 
  alist(
    happiness ~ dnorm( mu , sigma ), 
    mu <- a + bA*A,
    a ~ dnorm( 0 , 1 ),
    bA ~ dnorm( 0 , 2 ),
    sigma ~ dexp(1) ) , 
  data=d2 )
precis(m6.10)
```

No association if marriage not included. Highlights selection bias - more people get married as time goes on, so mean among married people approaches population average of zero; inverse for unmarried, happier people migrate over to married population.


### The haunted DAG

Not always easy to see a collider because of unmeasured causes, which can still induce it. Example - infer influence of parents $P$ and grandparents $G$ on education of children $C$. Think a simple dag where $G$ influences $P$ which both influence $C$. But possibly there's an unobservable $U$ confounder that's not shared by grandparents - say a neighborhood effect after parents moved

```{r, fig.height=2}
parent_dag <- dagitty( "dag {
  U [unobserved]
  C <- G -> P
  P -> C
  P <- U -> C
}")
coordinates( parent_dag ) <- list( x=c(G=0,P=1,C=1,U=2) ,
                                     y=c(G=0,P=0,C=1,U=0.5) )
drawdag( parent_dag )
```
  
  
  If conditioned on $P$, bias inference on $G \rightarrow$ C$, even without measureing $U$. Example:
  
```{r}
N <- 200 # number of grandparent-parent-child triads
b_GP <- 1 # direct effect of G on P
b_GC <- 0 # direct effect of G on C
b_PC <- 1 # direct effect of P on C
b_U<-2 #direct effect of U on P and C
  
set.seed(1)
U <- 2*rbern( N , 0.5 ) - 1
G <- rnorm( N )
P <- rnorm( N , b_GP*G + b_U*U )
C <- rnorm( N , b_PC*P + b_GC*G + b_U*U ) 
d <- data.frame( C=C , P=P , G=G , U=U )
```

Set $G$ effect as 0 for effect, $U$ is binary. Try to measure grandparent influence now

```{r}
m6.11 <- quap( 
  alist(
    C ~ dnorm( mu , sigma ), 
    mu <- a + b_PC*P + b_GC*G,
    a ~ dnorm( 0 , 1 ), 
    c(b_PC,b_GC) ~ dnorm( 0 , 1 ), 
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.11)
```
    

Grandparents have huge negative effect, parents have twice as large as it should be. Regression not wrong but causal interpretation is. If you make a plot of $C$ vs $G$, there's 2 sets, one for good neighborhoods $U=1$ and one for bad $U=-1$ - but selecting those of similar those with similar education causes a negative trend. Knowing $P$, learning $G$ invisibly tells us about the neighborhood - how to solve? Measure $U$.

```{r}
m6.12 <- quap( 
  alist(
    C ~ dnorm( mu , sigma ),
    mu <- a + b_PC*P + b_GC*G + b_U*U, 
    a ~ dnorm( 0 , 1 ), 
    c(b_PC,b_GC,b_U) ~ dnorm( 0 , 1 ), 
    sigma ~ dexp( 1 )
  ), data=d ) 
precis(m6.12)
```

Which matches the simulated data. This effect is _Simpson's Paradox_ - adding a predictor can reverse direction of association between another predictor and outcome.

## 6.4  Confronting Confounding

Multiple regression can both help us deal with confounding, and cause it - controlling for the wrong variables ruins inference.

**Confounding** - an outcome $Y$ and predictor $X$ are not the same as if we experimentally determined $X$. Example, educations $E$, wages $W$. Many unobserved variables that affect both.

```{r, fig.height=2}
wage_dag <- dagitty( "dag {
  E <- U -> W
  E -> W
}")
coordinates( wage_dag ) <- list( x=c(E=0,U=1,W=2) ,
                                     y=c(E=1,U=0,W=1) )
drawdag( wage_dag )
```

Regress $W$ on $E$, $U$ is a confounder, since there are 2 paths connecting $E$ and $W$. "Paths" in this sense ignore direction of arrows, but still create a statistical association, just not a _causal_ one. 

How to isolate the causal path? **Experiment** - assign education levels at random, removes the influence of $U$ on $E$.

```{r, fig.height=2}
exp_dag <- dagitty( "dag {
  U -> W
  E -> W
}")
coordinates( exp_dag ) <- list( x=c(E=0,U=1,W=2) ,
                                     y=c(E=1,U=0,W=1) )
drawdag( exp_dag )
```


Other ways to do this though! Add $U$ to the model, condition on it. This _blocks_ the flow of information between $E$ and $W$ through $U$. Once you learn $U$, learning $E$ provides no additional information about $W$ - consider just $E \leftarrow U \rightarrow W$ in isolation.

(this is important and I want to better understand, so taking verbose notes on it)

Fleshing out - say $U$ is average wealth in a region. High wealth $U$ leads to more education $E$, leads to better jobs, leads to higher wages. Not knowing region, learning education $E$ gives information about wages $W$, since these are correlated. But after learning region, assuming no other path between $E$ and $W$, learning education tells you nothing about $W$ - blocking the path. Becomes a fork.

### Shutting the backdoor

Blocking paths in this sense is called "shutting the backdoor." Don't want spurious correlation sneaking in through non-causal paths - $E \leftarrow U \rightarrow W$ is a backdoor since it enters $E$ with an arrow and connects $E$ to $W$.

Causal DAGs make it possible to say which we must control for to shut backdoor paths, also which _not_ to. 4 possible variable relations.

1) $X \leftarrow Z \rightarrow Y$ known as the **Fork**. $Z$ is a common cause of $X$ and $Y$, causing correlation. Conditioning on $Z$, then learning $X$ tells us nothing about $Y$. $X$ and $Y$ are independent conditional on $Z$

2) $X \rightarrow Z \rightarrow Y$ known as the **Pipe**. Seen in plant growth example - treatment$X$ influences fungus $Z$, which influences growth $Y$. Conditioning on $Z$ blocks the $X \rightarrow Y$ path.

3) $X \rightarrow Z \leftarrow Y$ known as the **Collider**. Earlier in the chapter - no association between $X$ and $Y$ unless conditioning on $Z$, which would open the path.

4) The **Descendant**, shown in the DAG below. A variable influenced by another, conditioning on it is a weaker form of conditioning on the variable itself - in the DAG, conditioning on $D$ will control $Z$, they share some information. This partially opens the path from $X$ to $Y$ since $Z$ is a collider. In the pipe $Z$ scenario, conditioning on the descendant is weakly closing the pipe.


```{r, fig.height=2}
descendent <- dagitty( "dag {
  X -> Z
  Y -> Z
  Z -> D
}")
coordinates( descendent ) <- list( x=c(X=0,Z=1,D=1,Y=2) ,
                                     y=c(X=1,Z=0,D=0.8,Y=1) )
drawdag( descendent )
```


All DAGs are composed of these 4 relations. Here's the approach:

1. List all paths connecting cause $X$ and outcome $Y$.
2. Classify each as open or closed - open unless it contains a collider.
3. Identify which have backdoor paths - those with an arrow entering $X$.
4. Decide which variables to condition on to close backdoors.

Examples to follow.

### Two Roads

```{r, fig.height=3}
tworoads <- dagitty( "dag {
  U [unobserved]
  U <- A -> C
  Y <- X <- U  -> B <- C -> Y
}")
coordinates( tworoads ) <- list( x=c(U=0,X=0,A=1,B=1,C=2,Y=2) ,
                                     y=c(U=0.5,X=1.5,A=0,B=1,C=0.5,Y=1.5) )
drawdag( tworoads )
```


Exposure of interest $X$, outcome of interest $Y$, unobservable $U$, 3 covariates $A$, $B$, $C$.

Interested in bottom path from $X$ to $Y$, but two other paths - one through $U$, $A$, $C$ and one through $U$, $B$, $C$.

Consider $A$ path, no colliders, so a backdoor - need to condition on it.

$B$ path though has a collider, so need to not condition on it.

```{r}
adjustmentSets( tworoads , exposure="X" , outcome="Y" )
```

Daggity tool shows us options to condition on - in this case, $C$ is a better option since it helps with precision of $X \rightarrow Y$. $U$ would also work, but of course is unobserved.

### Backdoor waffles

Going back to waffle house example. want to find minimal set of covariates, and derive testable implications. Data cannot tell us DAG is right, but can tell us when wrong.

```{r}
waffle <- dagitty( "dag {
  A -> D
  A -> M -> D
  A <- S -> M
  S -> W -> D
}")
drawdag( waffle )
```

$S$ is southern, $M$ is marriage rate, $W$ is waffle houses, $D$ is divorce. Looking between $W$ and $D$, 3 paths - SMD, SAD, SAMD.

```{r}
adjustmentSets( waffle , exposure="W" , outcome="D" )
```

Can close all paths by conditioning on $S$ alone. This DAG assumes no unobserved confounds, unlikely for this sort of data. Looking at the testable implications, or _conditional independencies_

```{r}
impliedConditionalIndependencies(waffle)
```

- Median age of marriage is independent of waffle houses, conditioning on a state being in the south
- Divorce rate is independent of being in the south, conditioning on age, marriage rate, and number of waffle houses
- Marriage rate and waffle houses are independent, conditioning on being in the south

## 6.5 Summary

Multiple regression describes conditional associations, not causal influences. Common frustrations outlined - multicollinearity, post-treatment bias, collider bias. It is to reach causal valid causal inferences in the absence of experimentation.

# Chapter 16 - Generalized Linear Madness
```{r, include=FALSE}
library(rethinking)
library(dagitty)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

Applied statistics has to apply to all sciences, and is more vague about models, focusing on average performance. They're not credible "scientific" models, more akin to geocentric descriptions. 

There are problems with the GLM plus DAG approach, not everything can be modeled as a GLM (linear combination of variables mapped to non-linear outcome). Sometimes parameters are fixed by theory. GLMs failure is hard to notice and learn.

This chapter - using scientific context to provide a causal model, _bespoke_ statistical models. Off the shelf models interrupt expertise, tools forbid use of expert knowledge.

## 16.1 - Geometric People

Height/weight example - weight does not _cause_ height, if anything opposite. We'll try to do better by approximating the person as a cylinder.

### Scientific Model

Volume of a cylinder is

$$
  V= \pi r^2 h
$$

Don't know radius but assume it's a constant proportion of height, $r=ph$, so

$$
  V = \pi (ph)^2 h = \pi p^2 h^3
$$

Then say weight is a proportion of volume 

$$
  W = kV = k \pi p^2 h^3.
$$

### Statistical Model

\begin{align*}
  W &\sim \text{Log-Normal}(\mu_i, \sigma)\\
  \exp(\mu_i) &= k \pi p^2 h_i^3\\
  k &\sim \text{ some prior}\\
  p &\sim \text{ some prior}\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

The first line is the distribution of the outcome, weight. It is positive and continuous, so chose Log-Normal, aparameterized by mean, $\mu$. The median is $\exp(\mu_i)$, so this is what we assign. 

One big thing to note is that $k$ and $p$ are multiplied and we have no way to estimate anything other than their product - they're _not identifiable_. So we replace $kp^2$ with a new parameter $\theta$,

$$
  \exp (\mu_i) = \pi \theta h_i^3.
$$
An advantage here is that parameters have meanings, which we can use for priors. This is a bit harder now that we've put in $\theta$ though.

For $p$, it's the ratio of radius to height, $p=r/h$, so greater than 0. Definitely less than 1, people are usually less wide than tall, and probably less than 1/2, so we should put most of our density below that,

$$ p \sim \text{Beta}(2,18) $$
This way it's got mean $2/(2+18) = 0.1$.

Next $k$, proportion of volume to weight - really just translates measurement scales. If height is in cm and weight in kg, volume has units cm$^3$, so $k$ is $kg/cm^3$, or how many kilograms are in a cubic centemeter - something we could look up or measure on our own bodies.

If that's not a possibility though, can just get rid of measurement scales - divide height and weight by mean value:

```{r}
data(Howell1)
d <- Howell1

# Scale variables
d$w <- d$weight / mean(d$weight)
d$h <- d$height / mean(d$height)
```

These now have means of 1, so we let $w_i = h_i =1$, turning the formula to:

$$
  1 = k \pi p^2 1^3
$$

If we want $p < 0.5$, then $k>1$, so we constrain $k$ to be postivie with a prior mean around 2,

$$
  k \sim \text{Exponential}(0.5)
$$

Codifying all of this:

```{r,  results=FALSE, message=FALSE, warning=FALSE}
m16.1 <- ulam(
  alist(
    w ~ dlnorm( mu , sigma),
    exp(mu) <- 3.141593 * k * p^2 * h^3,
    p ~ beta(2,18),
    k ~ exponential(0.5),
    sigma ~ exponential(1)
  ), data=d, chains=2, cores=2, cmdstan = TRUE
)
```


```{r}
plot(precis(m16.1))
```

```{r}
pairs(m16.1)
```


There's a narrow curved ridge in the posterior where $k$ and $p$ combinations produce the same product. Because we have informative priors, we can fit anyway

No reason that $k$ and $p$ aren't also functions of height or age, so $k$ isn't necessarily a constant (think changes in relative muscle mass), nor $p$. Helps understand predictions, plotting the predictions across height.

```{r}
h_seq <- seq(from=0, to=max(d$h),length.out=30)
w_sim <- sim(m16.1, data=list(h=h_seq))
mu_mean <- apply(w_sim, 2, mean)
w_CI <- apply(w_sim, 2, PI)
plot( d$h, d$w, 
      xlim=c(0,max(d$h)),ylim=c(0,max(d$w)), 
      col=rangi2, lwd=2,
      xlab="height (scaled)", ylab="weight (scaled)"
      )
lines(h_seq, mu_mean)
shade(w_CI, h_seq)
```

The model gets the general scaling relationship right, though it's not great at low values - $p$ and $k$ might be different for children.
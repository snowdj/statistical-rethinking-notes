## Chapter 7 problems

```{r, include=FALSE}
library(rethinking)
```

__7E1.__

>State the three motivating criteria that define information entropy. Try to express each in your
own words.

- On a continuous scale, with consistent spacing
- Value scales with possibilities
- Additive for multiple events



__7E2.__

>Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?

Assume known $p_H=0.7$. Then Entropy is $H(p) = -(0.3)\log(0.3)+ 0.7\log(0.7)$.

```{r}
p<- c(0.3, 0.7)
-sum(p*log(p))
```


__7E3.__

>Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?

```{r}
p<- c(0.2, 0.25, .25, .3)
-sum(p*log(p))
```


__7E4.__

>Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?

```{r}
p<- c(1/3, 1/3, 1/3)
-sum(p*log(p))
```


__7M1.__

>Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one?

Defined in chapter, WAIC is a generality of DIC which is a generality of AIC. WAIC to DIC requires Gaussian posterior assumption; DIC to AIC requires flat priors.

__7M2.__

>Explain the difference between model selection and model comparison. What information is lost under model selection?

Model selection tosses other models for the most accurate one (or lowest divergence). Chapter states is loses information about relative model accuracy, which matters most when accuracies are close.

__7M3.__

>When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.

Must be fit to the same observations for an apples-to-apples comparison, else, errors and information criterion values are meaningless.

__7M4.__

>What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

More concentrated prior causes the model to be less flexible, which leads to less effective parameters, so $p_{WAIC}$ decreases.

__7M5.__

>Provide an informal explanation of why informative priors reduce overfitting.

Provide a constraint on how flexible the model is, doesn't overweight outliers.

__7M6.__

>Provide an informal explanation of why overly informative priors result in underfitting.

Too much constraint on model flexibility can cause it not to learn from data in any meaningful way.






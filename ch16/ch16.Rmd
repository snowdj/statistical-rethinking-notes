# Chapter 16 - Generalized Linear Madness
```{r, include=FALSE}
library(rethinking)
library(dagitty)
library(bayesplot)
library(posterior)
library(ggplot2)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

Applied statistics has to apply to all sciences, and is more vague about models, focusing on average performance. They're not credible "scientific" models, more akin to geocentric descriptions. 

There are problems with the GLM plus DAG approach, not everything can be modeled as a GLM (linear combination of variables mapped to non-linear outcome). Sometimes parameters are fixed by theory. GLMs failure is hard to notice and learn.

This chapter - using scientific context to provide a causal model, _bespoke_ statistical models. Off the shelf models interrupt expertise, tools forbid use of expert knowledge.

## 16.1 - Geometric People

Height/weight example - weight does not _cause_ height, if anything opposite. We'll try to do better by approximating the person as a cylinder.

### Scientific Model

Volume of a cylinder is

$$
  V= \pi r^2 h
$$

Don't know radius but assume it's a constant proportion of height, $r=ph$, so

$$
  V = \pi (ph)^2 h = \pi p^2 h^3
$$

Then say weight is a proportion of volume 

$$
  W = kV = k \pi p^2 h^3.
$$

### Statistical Model

\begin{align*}
  W &\sim \text{Log-Normal}(\mu_i, \sigma)\\
  \exp(\mu_i) &= k \pi p^2 h_i^3\\
  k &\sim \text{ some prior}\\
  p &\sim \text{ some prior}\\
  \sigma &\sim \text{Exponential}(1)
\end{align*}

The first line is the distribution of the outcome, weight. It is positive and continuous, so chose Log-Normal, parameterized by mean, $\mu$. The median is $\exp(\mu_i)$, so this is what we assign. 

One big thing to note is that $k$ and $p$ are multiplied and we have no way to estimate anything other than their product - they're _not identifiable_. So we replace $kp^2$ with a new parameter $\theta$,

$$
  \exp (\mu_i) = \pi \theta h_i^3.
$$
An advantage here is that parameters have meanings, which we can use for priors. This is a bit harder now that we've put in $\theta$ though.

For $p$, it's the ratio of radius to height, $p=r/h$, so greater than 0. Definitely less than 1, people are usually less wide than tall, and probably less than 1/2, so we should put most of our density below that,

$$ p \sim \text{Beta}(2,18) $$
This way it's got mean $2/(2+18) = 0.1$.

Next $k$, proportion of volume to weight - really just translates measurement scales. If height is in cm and weight in kg, volume has units cm$^3$, so $k$ is $kg/cm^3$, or how many kilograms are in a cubic centimeter - something we could look up or measure on our own bodies.

If that's not a possibility though, can just get rid of measurement scales - divide height and weight by mean value:

```{r}
data(Howell1)
d <- Howell1

# Scale variables
d$w <- d$weight / mean(d$weight)
d$h <- d$height / mean(d$height)
```

These now have means of 1, so we let $w_i = h_i =1$, turning the formula to:

$$
  1 = k \pi p^2 1^3
$$

If we want $p < 0.5$, then $k>1$, so we constrain $k$ to be positive with a prior mean around 2,

$$
  k \sim \text{Exponential}(0.5)
$$

Codifying all of this:

```{r,  results=FALSE, message=FALSE, warning=FALSE}
m16.1 <- ulam(
  alist(
    w ~ dlnorm( mu , sigma),
    exp(mu) <- 3.141593 * k * p^2 * h^3,
    p ~ beta(2,18),
    k ~ exponential(0.5),
    sigma ~ exponential(1)
  ), data=d, chains=2, cores=2, cmdstan = TRUE
)
```


```{r}
plot(precis(m16.1))
```

```{r}
pairs(m16.1)
```


There's a narrow curved ridge in the posterior where $k$ and $p$ combinations produce the same product. Because we have informative priors, we can fit anyway

No reason that $k$ and $p$ aren't also functions of height or age, so $k$ isn't necessarily a constant (think changes in relative muscle mass), nor $p$. Helps understand predictions, plotting the predictions across height.

```{r}
h_seq <- seq(from=0, to=max(d$h),length.out=30)
w_sim <- sim(m16.1, data=list(h=h_seq))
mu_mean <- apply(w_sim, 2, mean)
w_CI <- apply(w_sim, 2, PI)
plot( d$h, d$w, 
      xlim=c(0,max(d$h)),ylim=c(0,max(d$w)), 
      col=rangi2, lwd=2,
      xlab="height (scaled)", ylab="weight (scaled)"
      )
lines(h_seq, mu_mean)
shade(w_CI, h_seq)
```

The model gets the general scaling relationship right, though it's not great at low values - $p$ and $k$ might be different for children.

The key-line takeaway: using the scientific approach, parameters have biological meanings, and these give us useful hints.

### GLLM in Disguise

Consider model related to logarithm of weight:

$$
  \log(w_i) = \mu_i = \log(k\pi p^2 h_i^3)
$$

Since multiplication becomes addition on log scale, rewrite:

$$
  \log(w_i) = \log(k) + \log(\pi) + 2\log(p) + 3\log(h_i)
$$

Linear regression on the log scale! Then $3\log(h_i)$ is a predictor with fixed coefficient of 3 from theory  that we didn't need to estimate. This just emphasizes the strength of GLMs, a lot of natural relationships _are_ GLMs.

## 16.2 Hidden Minds and Observed Behavior

The _inverse problem_ is common in scientific inference, figuring out causes from observations. This chapter will go from one with developmental psychology - how to determine if children are are influenced by the majority. 629 children saw 4 choose among 3 boxes. In each trial one was a majority choice (3), one a minority (1), and one unselected.

```{r}
library(rethinking)
data(Boxes)
precis(Boxes)
```

Outcome is y, taking value 1, 2, 3 - 1 is unchosen, 2 indicates majority, 3 is minority. Then `majority_first` shows if the majority color was demonstrated before the minority. Use outcome to infer strategies to choose a color.

```{r}
table(Boxes$y) / length(Boxes$y)
```

This does _not_ mean 45% are choosing the strategy of just following the majority - others exist. Pick at random, for example.

### Scientific Model

Think generatively - simulate cases where half pick at random, half follow majority
```{r}
set.seed(5)
N <- 30 # number of children

# half are random
# sample from 1,2,3 at random for each
y1 <- sample( 1:3 , size=N/2 , replace=TRUE )

# half follow majority
y2 <- rep( 2 , N/2 )

# combine and shuffle y1 and y2
y <- sample( c(y1,y2) )

# count the 2s
sum(y==2)/N
```

In this case about 2/3 pick the majority color, but half are actually doing that.

Consider 5 possible behaviors:

1. Follow Majority
2. Follow Minority
3. Maverick: pick the unselected color
4. Random
5. Follow first: pick the one that was demonstrated first

Why these? They seem plausible 

### Statistical Model

Remember, they run in reverse of generative models. Can't directly measure strategy, but each has a probability of producing each choice. So compute probability of each choice given parameters specified by the probability of each strategy, then get posterior back. Before we can do this, need to enumerate parameters, assign priors, and figure out technical issues.

Unobserved variables are probabilities for each of the strategies, we use a _simplex_. A vector of values that sum to a value (usually one), which we can give a Dirichlet prior. Use a weak uniform prior:

$$
  p \sim \text{Dirichlet}([4,4,4,4,4]).
$$

This means we don't expect any more or less probable than any other, if you make them larger the prior starts to say we expect them actually equal.

Now the likelihood. For each choice, $y_i$, each strategy $s$ implies a probability of seeing $y_i$, call this $\Pr(y_i|s)$, the probability of the data, conditional on assuming a strategy $s$. To get the probability for each possible $S$, average over the simplex:

$$
  \Pr(y_i) = \sum_{s=1}^{5} p_s \Pr(y_i|s)
$$

This is the weighted average of probabilities of $y_i$ conditional on each strategy $s$. This _marginalizes_ out the unknown strategy. In statistical fashion,

\begin{align*}
  y_i &\sim \text{Categorical}(\theta)\\
  \theta_j &= \sum_{s=1}^5 p_s \Pr(j|s) \text{   for  } j=1\dots 3 \\
  p &\sim \text{Dirichlet}([4,4,4,4,4])
\end{align*}

Where $\theta$ holds the probability of each behavior conditional on $p$.

### Coding the statistical model

Writing directly into stan, I'll write the model as a string, save as a `.stan` file, then use cmdstan to pre-compile. 

```{r}
model_code = "
data{
  int N;
  int y[N];
  int majority_first[N];
}
parameters{
  simplex[5] p;
}
model{
  vector[5] phi;
  
  // prior
  p ~ dirichlet( rep_vector(4,5) );
  
  // probability of data
  for ( i in 1:N ) {
      if ( y[i]==2 ) phi[1]=1; else phi[1]=0; // majority
      if ( y[i]==3 ) phi[2]=1; else phi[2]=0; // minority
      if ( y[i]==1 ) phi[3]=1; else phi[3]=0; // maverick
      phi[4]=1.0/3.0;                         // random
      if ( majority_first[i]==1 )             // follow first
          if ( y[i]==2 ) phi[5]=1; else phi[5]=0;
      else
          if ( y[i]==3 ) phi[5]=1; else phi[5]=0;
      
      // compute log( p_s * Pr(y_i|s )
      for ( j in 1:5 ) phi[j] = log(p[j]) + log(phi[j]);
      // compute average log-probability of y_i
      target += log_sum_exp( phi );
  }
}
"
m16_2_file <-"m16_2.stan"
writeLines(model_code, m16_2_file)
m16_2_cmdstan <- cmdstan_model(m16_2_file)
```

This needs 3 blocks of code. First the _data block_, which names observed variables and types. Next, the _parameters block_, like data, but for unobserved variables. Third, the _model bock_, which is the heart of our model. First, define phi to hold probability calculations, then define prior dirichlet. Then the tough part is the probability of data - loop over rows, for each row `i`, assign conditional on strategy. Also need to include the $p$ parameters, by adding each $\log(p_i)$, done in the last chunk above.

 
Now prepping data and running sampler.

```{r, results=FALSE, message=FALSE, warning=FALSE}
# prep data
dat_list <- list(
  N = nrow(Boxes),
  y = Boxes$y,
  majority_first = Boxes$majority_first
)

# Run stan using model defined above
m16.2 <- m16_2_cmdstan$sample(data=dat_list, chains=2, cores=2)
```

```{r}
# Get posterior draws
m16_2_draws <- m16.2$draws()
```

```{r}
posterior <- as_draws_df(m16_2_draws)
plot_title <- ggtitle("Posterior distributions","of various strategies")
mcmc_areas(posterior, 
           pars=c("p[1]","p[2]","p[3]","p[4]","p[5]"), prob=.89) + plot_title
```

This has been sort of made in an ad-hoc way due to having to use cmdstan to pre-compile the model. 45% chose the majority color, but with this construction, between 20-30% were only doing it because of copying (p[1]).

### State Space Models

Boxes model shown represents a broader class called _state space models_ - multiple hidden states that produce observations, changing over time. When discrete categories, it may be called a _Hidden Markov Model_ (HMM). Applies to many time series models, since true state is not observed, just noisy measures.

## 16.3 - Ordinary Differential Nut Cracking

Next example will look at Panda nuts, which are difficult for animals to open, but manageable with tools - model the development of nut opening skill among chimpanzees.

```{r}
data(Panda_nuts)
d <- Panda_nuts
head(d)
```

Each row is an bout of nut opening, interested in `nuts_opened`, the duration in `seconds` and `age`.

### Scientific model

Most basic model - only thing that matters is strength, which increases with age. In animals with terminal growth (reach stable adult body mass), size increases proportionally with distance remaining to maximum size,

$$
  \frac{dM}{dt} = k(M_{\text{max}}-M_t).
$$

Here, $k$ is a parameter measuring rate of skill gain with age. This differential equation is common in biology, with solution,

$$
  M_t = M_{\text{max}}(1-\exp(-kt)).
$$

What we care about though is strength, so we need to suppose strength is proportional to mass: $S_t = \beta M_t$, with $\beta$ as a constant of proportionality. 

Last, need to relate strength to rate of nut cracking. It helps in 3 ways - heavier usable tools, faster arm acceleration, more efficient lever arms. This implies increasing returns,

$$
  \lambda = \alpha S_t^\theta = \alpha(\beta M_{\text{max}}(1-\exp(-kt)))^\theta,
$$

with $\theta$ being an exponent larger than 1. The new $\alpha$ parameter expresses proportionality of strength to nut opening - newtons of force to nuts per second.

This is a giant soup of parameters, so let's simplify. First, rescale body mass scale, $M_{max}$, to equal 1. Next, $\alpha \beta^\theta$ is a giant scaling factor, so you can replace it with a single parameter $\phi$, yielding

$$
  \lambda = \phi (1-\exp(-kt))^\theta.
$$


### Statistical Model

Likelihood is straightforward, it's just Poisson distributed.

\begin{align*}
  n_i &\sim \text{Poisson}(\lambda_i)\\
  \lambda_i &= d_i \phi(1-\exp(-kt_i))^\theta
\end{align*}

Our outcome is $n_i$, number of nuts cracked, and the equation for $\lambda_i$ is the derived equation for rate of nuts cracked multiplied by duration/exposure $d_i$.

Priors require us to consider biology. Chimpanzees reach adult mass around 12 years of age, $k$ and $\theta$ need to accomplish this. $\phi$ needs to have a mean around the maximum rate... who really knows, but hazarding a guess, one per second, since multiple can be opened at once? Let's try:

\begin{align*}
  \phi &\sim \text{Log-Normal}(\log(1),0.1)\\
  k &\sim \text{Log-Normal}(\log(2),0.25)\\
  \theta &\sim \text{Log-Normal}(\log(5),0.25)
\end{align*}

All are set to log normal because they're both positive and continuous. Let's do a simulation test:

```{r}
N <- 1e4
phi <- rlnorm(N, log(1), 0.1)
k <- rlnorm(N, log(2), 0.25)
theta <- rlnorm(N, log(5), 0.25)

# Growth curve
plot(NULL, xlim=c(0,1.5), ylim=c(0,1), 
     xaxt="n", xlab="Age", ylab="Body Mass")
at <- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5)
axis(1, at=at, labels=round(at*max(Panda_nuts$age)))
for (i in 1:20) curve ( (1-exp(-k[i]*x)), add=TRUE, col=grau(),lwd=1.5)
```

```{r}
# Implied rate of nut opening curve
plot(NULL, xlim=c(0,1.5), ylim=c(0,1.2), 
     xaxt="n", xlab="Age", ylab="Nuts/Second")
at <- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5)
axis(1, at=at, labels=round(at*max(Panda_nuts$age)))
for (i in 1:20) curve ( phi[i]*(1-exp(-k[i]*x))^theta[i], add=TRUE, col=grau(),lwd=1.5)
```


Coding this model is possible with ulam,

```{r, results=FALSE, message=FALSE, warning=FALSE}
dat_list <- list(
  n = as.integer( Panda_nuts$nuts_opened),
  age = Panda_nuts$age / max(Panda_nuts$age),
  seconds = Panda_nuts$seconds
)

m16.4 <- ulam(
  alist(
    n ~ poisson(lambda),
    lambda <- seconds * phi * (1-exp(-k*age))^theta,
    phi ~ lognormal(log(1),0.1),
    k ~ lognormal(log(2),0.25),
    theta ~ lognormal(log(5),0.25)
  ), data=dat_list, chains=2, cores=2, cmdstan = TRUE
)
```


Interesting thing is the posterior developmental curve,

```{r}
post <- extract.samples(m16.4)
plot(NULL, xlim=c(0,1.5), ylim=c(0,1.5), 
     xaxt="n", xlab="Age", ylab="Nuts/Second")
at <- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5)
axis(1, at=at, labels=round(at*max(Panda_nuts$age)))

# Raw data
pts <- dat_list$n / dat_list$seconds
point_size <- normalize(dat_list$seconds)
points( jitter(dat_list$age), pts, col=rangi2, lwd=2, cex=point_size*3)

# Posterior curves
for (i in 1:30) with (post,
                      curve( phi[i]*(1-exp(-k[i]*x))^theta[i], add=TRUE, col=grau() ))
```

Blue points are raw data with size scaled by duration. 30 skill curves drawn from posterior distribution. They level off around the age of maximum body size, which is consistent with the idea that strength is the limiting factor.


### Covariates and individual differences

This model is pretty simple - you could extend it to covariates like sex. There are repeat observations of individuals, even across years, which could be used to estimate individual varying effects. You could also include a more realistic growth model of chimpanzees, which is well published.

Some parameters make sense varying by individual, others don't. $\theta$ is a feature of only physics, not individual, probably don't vary it - this is another reason to avoid GLMs.

## 16.4 - Population Dynamics

Populations of hares and lynx fluctuate over time - understanding number at a given time helps to understand in the future. We'll model a _time series_ of populations.

```{r}
data(Lynx_Hare)
plot( 1:21, Lynx_Hare[,3], ylim=c(0,90),
      xlab="year", ylab="Thousands of Pelts",
      xaxt="n", type="l", lwd=1.5)
at <- c(1,11,21)
axis <- c(1, at=at, labels=Lynx_Hare$Year[at])
lines(1:21, Lynx_Hare[,2], lwd=1.5, col=rangi2)
points(1:21, Lynx_Hare[,3], bg="Black", col="white", pch=21, cex=1.4)
points(1:21, Lynx_Hare[,2], bg=rangi2, col="white", pch=21, cex=1.4)
text( 17, 80, "Lepus", pos=2)
text(19,50, "Lynx", pos=2, col=rangi2)
```

Note, these are records of pelts, not live animals. Fluctuations appear to happen in tandem.

A common way to model time series is an _auto-regressive model_, where the outcome in the previous time step is a _lag variable_ and added as a predictor to the next time step, for example, hares at time $t$,

$$
  E(H_t) = \alpha + \beta_1 H_{t-1} + \beta_2 L_{t-1}
$$

Where $H_t$ is the hares at time $t$. If $\beta_1$ is less than $1$, they regress to a mean population of $\alpha$. The third term is a predator effect, the number of lynx at the previous time period. Some even add a deeper lag, a $\beta_3 H_{t-2}$ term.

Several famous problems use auto-regressive models, but they're not great true models - nothing that happened 2 steps ago really matter now. These models also propagate error, the observed value at prior time step doesn't affect real life, only the true, unobserved, value. What we really need is a _state space model_. Last, there's no physical interpretation of the parameters. Sometimes these problems are fine if you're forecasting, but for anything else, they're not. And often they're even bad at forecasting.

This section will also go over how to fit systems of _ordinary differential equations_ (ODEs) to data.

### Scientific Model

Hare population is dependent on plants, shrinks at a rate dependent on number of predators. $H_t$ is hares at time $t$, so let the rate of change in hare population be

$$
  \frac{dH}{dt} = H_t(\text{birth rate}) - H_t(\text{death rate})
$$

Multiply by $H_t$ because it's a per capita process. Simplest option: let both birth rate, $b_H$, and mortality rate $m_H$ be constant.

$$
  \frac{dH}{dt} = H_t b_H - H_t m_H = H_t(b_H -m_H)
$$

The per capita growth rate is the difference between birth and death rate (very common in ecology).

Next, we want to make the mortality rate dependent on presence of a predator, the lynx. Let their population at time $t$ be $L_t$, giving

$$
  \frac{dH}{dt} = H_t(b_H - L_t m_H)
$$

The rate of change of the lynx population is similar, but we have births dependent on the other species,

$$
  \frac{dL}{dt} = L_t (H_t b_L - m_L).
$$

This model gives two coupled ODEs, but isn't fully realistic - lynx eat more than just hares and it acts as though plants are infinitely available.

Model is known as the _Lotka-Volterra Model_, modeling simple predator-prey relationships. Even without fitting data, gives some insights - oscillatory behavior in populations.

This case, no explicit solution to solve for populations, but can solve numerically. An ODE can be thought of as a way to update a variable - how to update $H$ at $t$ time steps:

$$
  H_{t+dt} = H_t + dt \frac{dH}{dt} = H_t + dt H_t (b_H - L_t m_H)
$$

Need to be a bit careful (small enough time steps) but ought to work,

```{r}
sim_lynx_hare <- function(n_steps , init, theta, dt=0.002){
  L <- rep(NA, n_steps)
  H <- rep(NA, n_steps)
  L[1] <- init[1]
  H[1] <- init[2]
  for (i in 2:n_steps){
    H[i] <- H[i-1] + dt*H[i-1]*(theta[1] - theta[2]*L[i-1])
    L[i] <- L[i-1] + dt*L[i-1]*(theta[3]*H[i-1]-theta[4])
  }
  return(cbind(L,H))
}
```

That function should simulate the ODE, next need to run simulation,

```{r}
theta <- c(0.5, 0.05, 0.025, 0.5)
z <- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1,2:3]),theta)
plot( z[,2], type="l", 
      ylim=c(0,max(z[,2])), lwd=2, xaxt="n",
      ylab="number (thousands)", xlab="")
lines(z[,1], col=rangi2, lwd=2)
mtext("Time",1)
```

Black curve is hare population, blue is lynx, cyclic behavior is observed.

### Statistical Model

Need to connect population dynamics to observed data. Don't match one-to-one, only have partial samples, so need to model population dynamics and observation process.

Introduce new variables $h_t$ and $\ell_t$, the observed number of each population. $H_t$ causes $H_{t+dt}$, but $h_t$ doesn't directly cause anything. Can then assign an error distribution to the observation process, but should do in a principled manner.

Say hares get trapped at a probability $p_t$, varies year to year, and pelts were rounded to the nearest 100 and divided by 1000, so not exactly "counts" - this is what real measurement may look like. So we may assign trapping rate a beta distribution with $p_t \sim \text{Beta}(2,18)$. Get a binomial count of number of pelts, then we can round accordingly.

```{r}
N <- 1e4
Ht <- 1e4
p <- rbeta(N,2,18)
h <- rbinom(N, size=Ht, prob=p)
h <- round(h/1000, 2)
dens(h, xlab="Thousand of Pelts", lwd=2)
```

There are several ways to approximate - Log-Normal has the right constraints and skew,

$$
  h_t \sim \text{Log-Normal}(\log(pH_t), \sigma_H)
$$

Gives $h_t$ a median of $pH_t$, the expected proportion of trapped hare population, with dispersion $\sigma_H$. No great way to estimate $p$ short of a lot of data, so fix it with a strong prior - model has forced us to realize that we cannot do any better than relative population estimates. Ignore rounding error, it's of order 1.25%. Full model:

\begin{align*}
  h_t &\sim \text{Log-Normal}(\log(p_H H_t), \sigma_H)\\
  \ell_t &\sim \text{Log-Normal}(\log(pL L_t), \sigma_L)\\
  H_1 &\sim \text{Log-Normal}(\log(10),1)\\
  l_1 &\sim \text{Log-Normal}(\log(10),1)\\
  H_{T>1} &= H_1 + \int_1^T H_t(b_H - m_H L_t)dt\\
  L_{T>1} &= L_1 + \int_1^T L_t(b_L H_t - m_Ll)dt
\end{align*}

So the first two lines are the probability of observed hare and lynx pelts, the next two are the initialization priors for each population, then the following define times afterwards. Last we need a lot of priors:

\begin{align*}
  \sigma_H &\sim \text{Exponential}(1)\\
  \sigma_L &\sim \text{Exponential}(1)\\
  p_H &\sim \text{Beta}(\alpha_H, \beta_H)\\
  p_L &\sim \text{Beta}(\alpha_L, \beta_L)\\
  b_H &\sim \text{Half-Normal}(1, 0.5)\\
  b_L &\sim \text{Half-Normal}(0.05, 0.05)\\
  m_H &\sim \text{Half-Normal}(0.05,0.05)\\
  m_L &\sim \text{Half-Normal}(1,0.05)\\  
\end{align*}


Next we'll put this all into stan, which luckily already has functions for solving ODEs.

```{r}
model_code <- "
functions {
  real[] dpop_dt( real t,                 // time
                real[] pop_init,          // initial state {lynx, hares}
                real[] theta,             // parameters
                real[] x_r, int[] x_i) {  // unused
    real L = pop_init[1];
    real H = pop_init[2];
    real bh = theta[1];
    real mh = theta[2];
    real ml = theta[3];
    real bl = theta[4];
    // differential equations
    real dH_dt = (bh - mh * L) * H;
    real dL_dt = (bl * H - ml) * L;
    return { dL_dt , dH_dt };
  }
}
data {
  int<lower=0> N;              // number of measurement times
  real<lower=0> pelts[N,2];    // measured populations
}
transformed data{
  real times_measured[N-1];    // N-1 because first time is initial state
  for ( i in 2:N ) times_measured[i-1] = i;
}
parameters {
  real<lower=0> theta[4];      // { bh, mh, ml, bl }
  real<lower=0> pop_init[2];   // initial population state
  real<lower=0> sigma[2];      // measurement errors
  real<lower=0,upper=1> p[2];  // trap rate
}
transformed parameters {
  real pop[N, 2];
  pop[1,1] = pop_init[1];
  pop[1,2] = pop_init[2];
  pop[2:N,1:2] = integrate_ode_rk45(
    dpop_dt, pop_init, 0, times_measured, theta,
    rep_array(0.0, 0), rep_array(0, 0),
    1e-5, 1e-3, 5e2);
}
model {
  // priors
  theta[{1,3}] ~ normal( 1 , 0.5 );    // bh,ml
  theta[{2,4}] ~ normal( 0.05, 0.05 ); // mh,bl
  sigma ~ exponential( 1 );
  pop_init ~ lognormal( log(10) , 1 );
  p ~ beta(40,200);
  // observation model
  // connect latent population state to observed pelts
  for ( t in 1:N )
    for ( k in 1:2 )
      pelts[t,k] ~ lognormal( log(pop[t,k]*p[k]) , sigma[k] );
}
generated quantities {
  real pelts_pred[N,2];
  for ( t in 1:N )
    for ( k in 1:2 )
      pelts_pred[t,k] = lognormal_rng( log(pop[t,k]*p[k]) , sigma[k] );
}
"
m16_5_file <-"m16_5.stan"
writeLines(model_code, m16_5_file)
m16_5_cmdstan <- cmdstan_model(m16_5_file)
```

This uses the useful `integrate_ode_rk45` function to solve the ODE, and otherwise is relatively straightforward. A double `for` loop relates solved equations to data. Next we'll fit

```{r,results=FALSE, message=FALSE, warning=FALSE}
# prep data
dat_list <- list(
  N = nrow(Lynx_Hare), 
  pelts = Lynx_Hare[,2:3] 
  )

# Run stan using model defined above
m16.5 <- m16_5_cmdstan$sample(data=dat_list, chains=2, parallel_chains = 2)
```

Get posterior draws,

```{r}
m16_5_draws <- m16.5$draws()
posterior_df <- as_draws_df(m16_5_draws)
posterior_matrix <- as_draws_matrix(m16_5_draws)
```

Then plot posterior prediction of pelts,

```{r, warning=FALSE}
pelts <- dat_list$pelts
plot( 1:21 , pelts[,2] , pch=16 , ylim=c(0,120) , xlab="year" ,
    ylab="thousands of pelts" , xaxt="n" )
at <- c(1,11,21)
axis( 1 , at=at , labels=Lynx_Hare$Year[at] )
points( 1:21 , pelts[,1] , col=rangi2 , pch=16 )
# 21 time series from posterior
hares_only_matrix <- posterior_matrix[,75:95]
lynx_only_matrix <- posterior_matrix[,54:74]
for ( s in 1:21 ) {
    lines( 1:21 , hares_only_matrix[s,] , col=col.alpha("black",0.2) , lwd=2 )
    lines( 1:21 , lynx_only_matrix[s,] , col=col.alpha(rangi2,0.3) , lwd=2 )
}
# text labels
text( 17 , 90 , "Lepus" , pos=2 )
text( 19 , 50 , "Lynx" , pos=2 , col=rangi2 )
```


Trends are jagged, a result of uncorrelated measurement errors - underlying population is smooth but measurements may not be. Helpful to compare the pelt predictions to the population predictions.


```{r, warning=FALSE}
plot( NULL , pch=16 , xlim=c(1,21) , ylim=c(0,500) , xlab="year" ,
    ylab="thousands of animals" , xaxt="n" )
at <- c(1,11,21)
axis( 1 , at=at , labels=Lynx_Hare$Year[at] )
hares_only_matrix <- posterior_matrix[,33:53]
lynx_only_matrix <- posterior_matrix[,12:32]
for ( s in 1:21 ) {
    lines( 1:21 , hares_only_matrix[s,] , col=col.alpha("black",0.2) , lwd=2 )
    lines( 1:21 , lynx_only_matrix[s,] , col=col.alpha(rangi2,0.4) , lwd=2 )
}
text( 17 , 450 , "Lepus" , pos=2 )
text( 19.6 , 200 , "Lynx" , pos=2 , col=rangi2 )
```

### Lynx Lessons

Obviously not a perfect model, lynx eat more than hares, hares have other predators, so this probably isn't the true cause - there's some lurking confound. Real ecology is complicated.

## 16.5 - Summary

Demonstrated 4 analyses in which a statistical model is motivated by a scientific one - this is in contrast to generic GLMS. The goal is to illustrate advantages and difficulties from translating into statistical machines. **Using a GLM is not an obligation, but a decision in itself.**

